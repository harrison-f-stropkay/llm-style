{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "- using longest responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN = \"human\"\n",
    "\n",
    "GEMINI_10 = \"gemini-1.0-pro\"\n",
    "GEMINI_15 = \"gemini-1.5-pro-latest\"\n",
    "\n",
    "CLAUDE_SONNET = \"claude-3-sonnet-20240229\"\n",
    "CLAUDE_OPUS = \"claude-3-opus-20240229\"\n",
    "\n",
    "GPT_35 = \"gpt-3.5-turbo-0125\"\n",
    "GPT_40 = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "\n",
    "authors = [HUMAN, GEMINI_10, GEMINI_15, CLAUDE_SONNET, CLAUDE_OPUS, GPT_35, GPT_40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"function_words/oshea.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    fw_oseah = [line.split()[0] for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z-Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ZScoreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, function_words):\n",
    "        self.function_words = function_words\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            use_idf=False, norm=None, tokenizer=word_tokenize, token_pattern=None\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the vectorizer to the corpus\n",
    "        word_counts = self.vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "        # oov = [\n",
    "        #    word\n",
    "        #    for word in self.function_words\n",
    "        #    if word not in self.vectorizer.vocabulary_\n",
    "        # print(f\"{len(oov)} words not in corpus: {oov}\")\n",
    "\n",
    "        # Fit z-score scaler to the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "        self.scaler.fit(relative_freqs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform the corpus into word counts\n",
    "        word_counts = self.vectorizer.transform(X).toarray()\n",
    "\n",
    "        # Calculate the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Calculate the z-scores\n",
    "        z_scores = self.scaler.transform(relative_freqs)\n",
    "\n",
    "        fw_indices = [\n",
    "            self.vectorizer.vocabulary_[word]\n",
    "            for word in self.function_words\n",
    "            if word in self.vectorizer.vocabulary_\n",
    "        ]\n",
    "\n",
    "        # Keep the relative frquencies of only the function words\n",
    "        fw_z_scores = z_scores[:, fw_indices]\n",
    "\n",
    "        return fw_z_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(z_scores, variant):\n",
    "    if variant == \"burrows\":\n",
    "        return cdist(z_scores, z_scores, metric=\"cityblock\")\n",
    "    if variant == \"cosine\":\n",
    "        return cdist(z_scores, z_scores, metric=\"cosine\")\n",
    "\n",
    "    raise ValueError(f\"Unknown variant: {variant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def classify(df, function_words, test_prompts=None, print_n_most_important=None):\n",
    "    # Train-test split\n",
    "    prompt_ids = df[\"prompt_id\"].unique().tolist()\n",
    "    if not test_prompts:\n",
    "        test_prompts = random.sample(prompt_ids, 2)\n",
    "\n",
    "    df_test = df[df[\"prompt_id\"].isin(test_prompts)].reset_index(drop=True)\n",
    "    df_train = df[~df[\"prompt_id\"].isin(test_prompts)].reset_index(drop=True)\n",
    "\n",
    "    # Use the ZScoreTransformer to get the z-scores\n",
    "    z_scores_transformer = ZScoreTransformer(function_words)\n",
    "    z_scores_train = z_scores_transformer.fit_transform(df_train[\"text\"])\n",
    "    z_scores_test = z_scores_transformer.transform(df_test[\"text\"])\n",
    "\n",
    "    # Set up cross-validation\n",
    "    train_indices_by_prompt = (\n",
    "        df_train.groupby(\"prompt_id\")\n",
    "        .apply(lambda x: x.index, include_groups=False)\n",
    "        .tolist()\n",
    "    )\n",
    "    cv_iterable = []\n",
    "    for _ in range(3):\n",
    "        train_indices_by_prompt = (\n",
    "            train_indices_by_prompt[2:] + train_indices_by_prompt[:2]\n",
    "        )\n",
    "        val_indices = np.concatenate(train_indices_by_prompt[:2])\n",
    "        train_indices = np.concatenate(train_indices_by_prompt[2:])\n",
    "        cv_iterable.append((train_indices, val_indices))\n",
    "\n",
    "    # Train the model using grid search\n",
    "    model = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        param_grid={\n",
    "            \"C\": [0.001, 0.003, 0.01, 0.03, 0.1],\n",
    "            \"solver\": [\"liblinear\", \"lbfgs\"],\n",
    "        },\n",
    "        cv=cv_iterable,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "    )\n",
    "\n",
    "    model.fit(z_scores_train, df_train[\"author\"])\n",
    "    y_pred = model.predict(z_scores_test)\n",
    "\n",
    "    df_test[\"y_pred\"] = y_pred\n",
    "\n",
    "    # print the words that were most important for the model\n",
    "    if print_n_most_important:\n",
    "        coefs = model.best_estimator_.coef_\n",
    "        coefs = coefs[0]\n",
    "        coefs = np.abs(coefs)\n",
    "        coefs = coefs / coefs.sum()\n",
    "        fw = np.array(function_words)\n",
    "        fw = fw[coefs.argsort()[::-1]]\n",
    "        print(fw[:print_n_most_important])\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most important words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ([\"reddit\"] * 5) + ([\"hewlett\"] * 5):\n",
    "    df_human = pd.read_csv(\"reddit_responses/human.csv\")\n",
    "    df_human[\"author\"] = HUMAN\n",
    "    df_gpt = pd.read_csv(\"reddit_responses/gpt-3.5-turbo-0125.csv\")\n",
    "    df_gpt[\"author\"] = GPT_35\n",
    "    df = pd.concat([df_human, df_gpt], ignore_index=True)\n",
    "\n",
    "    classify(df, fw_oseah, print_n_most_important=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies: he, she, a, the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit\n",
      "he: human: 0.006625333485078846 gpt: 0.0021662682621693243\n",
      "she: human: 0.004413168826978141 gpt: 0.012964098128293942\n",
      "a: human: 0.014263125695069606 gpt: 0.027195232612212766\n",
      "the: human: 0.034400312602841265 gpt: 0.06502085149060624\n",
      "\n",
      "hewlett\n",
      "he: human: 0.005037284925477697 gpt: 0.001965197994441381\n",
      "she: human: 0.004390490978073949 gpt: 0.0018364145447078678\n",
      "a: human: 0.017954408912960206 gpt: 0.022107849185965252\n",
      "the: human: 0.04745341423057013 gpt: 0.06075187005456569\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset in [\"reddit\", \"hewlett\"]:\n",
    "    print(dataset)\n",
    "    df_human = pd.read_csv(f\"{dataset}_responses/human.csv\")\n",
    "    df_gpt = pd.read_csv(f\"{dataset}_responses/gpt-3.5-turbo-0125.csv\")\n",
    "\n",
    "    df_human[\"tokens\"] = df_human[\"text\"].apply(word_tokenize)\n",
    "    df_gpt[\"tokens\"] = df_gpt[\"text\"].apply(word_tokenize)\n",
    "\n",
    "    # see the relative frequency of 'he' and 'she' in GPT-3.5 vs human\n",
    "    words = [\"he\", \"she\", \"a\", \"the\"]\n",
    "    for word in words:\n",
    "        word_rel_freq_gpt = df_gpt[\"tokens\"].apply(lambda x: x.count(word) / len(x))\n",
    "        word_rel_freq_human = df_human[\"tokens\"].apply(lambda x: x.count(word) / len(x))\n",
    "        print(\n",
    "            f\"{word}: human: {word_rel_freq_human.mean()} gpt: {word_rel_freq_gpt.mean()}\"\n",
    "        )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass (all 7 authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 10\n",
    "\n",
    "for dataset in [\"reddit\", \"hewlett\"]:\n",
    "    dfs = []\n",
    "    for author in authors:\n",
    "        df = pd.read_csv(f\"{dataset}_responses/{author}.csv\")\n",
    "        df[\"author\"] = author\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    avg_conf_matrix = np.zeros((len(authors), len(authors)))\n",
    "    for _ in tqdm(range(n_trials), desc=f\"Running {dataset} trials\"):\n",
    "        df_test = classify(df, fw_oseah)\n",
    "        conf_matrix = confusion_matrix(df_test[\"author\"], df_test[\"y_pred\"])\n",
    "        avg_conf_matrix += conf_matrix\n",
    "\n",
    "    avg_conf_matrix /= n_trials\n",
    "\n",
    "    sns.heatmap(\n",
    "        avg_conf_matrix,\n",
    "        annot=True,\n",
    "        cmap=\"YlGnBu\",\n",
    "        fmt=\".1f\",\n",
    "        cbar_kws={\n",
    "            \"label\": \"Number of Predictions\",\n",
    "            \"boundaries\": np.arange(0, 201, 1),\n",
    "            \"ticks\": np.arange(0, 201, 50),\n",
    "        },\n",
    "        xticklabels=authors,\n",
    "        yticklabels=authors,\n",
    "    )\n",
    "    plt.title(f\"Average Confusion Matrix: {dataset.capitalize()} dataset\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise with human and each LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "n_trials = 10\n",
    "\n",
    "llms = [GEMINI_10, GEMINI_15, CLAUDE_SONNET, CLAUDE_OPUS, GPT_35, GPT_40]\n",
    "\n",
    "for dataset in [\"reddit\", \"hewlett\"]:\n",
    "    results = []\n",
    "    for llm in tqdm(llms * n_trials):\n",
    "        df_llm = pd.read_csv(f\"{dataset}_responses/{llm}.csv\")\n",
    "        df_llm[\"author\"] = llm\n",
    "        df_human = pd.read_csv(f\"{dataset}_responses/{HUMAN}.csv\")\n",
    "        df_human[\"author\"] = HUMAN\n",
    "        df = pd.concat([df_llm, df_human], ignore_index=True)\n",
    "\n",
    "        df_test = classify(df, fw_oseah)\n",
    "        report = classification_report(\n",
    "            df_test[\"author\"], df_test[\"y_pred\"], zero_division=0, output_dict=True\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"dataset\": dataset,\n",
    "                \"llm\": llm,\n",
    "                \"family\": llm.split(\"-\")[0],\n",
    "                \"accuracy\": report[\"accuracy\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=results_df, x=\"llm\", y=\"accuracy\", hue=\"family\")\n",
    "    plt.title(\n",
    "        f\"Human-LLM Binary Logistic Regression Accuracy: {dataset.capitalize()} dataset\"\n",
    "    )\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"LLM\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mis-labels: GPT 3.5, human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:30<00:00, 30.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: reddit\n",
      "Prompt: 0\n",
      "Human predicted LLM: N/A\n",
      "LLM predicted human: In a distant corner of the galaxy, the peaceful alien race known as the Altarians found themselves facing an unprecedented threat from the ruthless Xerathians. Despite their best efforts to defend their planets, the Altarians were slowly being pushed to the brink of extinction.\n",
      "\n",
      "With no other options left, the Altarians made a bold and desperate decision – they reached out to the humans, a notoriously brutal and advanced species known for their capability for war and destruction. The Altarians offered the humans something they desired greatly: FTL (faster-than-light) technology, in exchange for their assistance in repelling the Xerathian invasion.\n",
      "\n",
      "The decision was met with skepticism and apprehension among the humans, as they were not known for being saviors or protectors. However, the allure of expanding their reach across the galaxy with FTL technology was too great to ignore. Ultimately, the humans chose to accept the Altarians' offer and prepared to join the fight against the Xerathians.\n",
      "\n",
      "As the humans arrived with their advanced weaponry and tactics, they quickly turned the tide of the battle in favor of the Altarians. With their combined forces, they launched a daring and ruthless counterattack that caught the Xerathians off guard. The once invincible invaders were pushed back, planet by planet, until they were finally driven out of Altarian space.\n",
      "\n",
      "Although the humans had brought destruction and chaos with them, their intervention had saved the Altarians from certain annihilation. The two races had forged an uneasy alliance, bound by the shared experience of war and survival.\n",
      "\n",
      "In the end, the Altarians had sacrificed their peaceful ways to ensure their survival, while the humans had proven that even the most brutal creatures could show mercy and compassion in the face of a common enemy. The galaxy would never be the same again, but perhaps, in this unlikely partnership, there was hope for a better future.\n",
      "\n",
      "Prompt: 1\n",
      "Human predicted LLM: N/A\n",
      "LLM predicted human: N/A\n",
      "\n",
      "Prompt: 2\n",
      "Human predicted LLM: **The Tribe** <newline> <newline> They hunt in the fields and in the woods , stitching together clothes of animal hide and making spears and bows . They fish in the rivers , fashioning themselves nets and hooks . They scavenge in the forests for roots , nuts , and berries . They move with the changing of the seasons . <newline> <newline> <newline> Their children play together , carefree , running and jumping . They roll around in the tall grass and splash in the shallow pools . They help in the finding of food and the building of tools . The whole tribe is content . <newline> <newline> <newline> Their way of life is a simple one , of hunting and gathering , eating and sleeping , always on the move . They live as a tribe , enjoying the simple pleasures of living and the quiet beauty of the natural world . And at night , when the shadows grow long and the sun nears the horizon , they tell each other stories . <newline> <newline> <newline> The tribe elders tell the stories of an ancient and great civilization whose cities rose to the sky and whose wonders were unparalleled . They tell the stories of a people , buoyed up by their pride and creations , to become gods on the earth . They tell stories of an age long gone . They speak of wars and wonders . <newline> <newline> <newline> The tribe elders tell the stories of great heroes and conquerors . They tell of wars that leveled entire cities , of armies millions strong . The tribe elders speak gravely their myths of death and destruction . They speak of a war that split the world apart , bringing the age of that civilization to an end . <newline> <newline> <newline> The children grow up in the shadow of the great ruins . They explore the husks of ancient decrepit towers and cities . They play in the long-dead corpses of the ancient metal war-machines , long silenced . The tribe scavenges from the ruins spare bits of metal , spare bits of cloth , ancient relics to make tools and clothes . The crown of a king who once commanded the entire world is now a speartip for one of the tribe ’ s young boys . The torn robe of a general who once commanded twelve million souls is now the swaddling cloth for one of the tribe ’ s newborn . The ancient power of the people 's ’ past is felt in nothing but relics and stories . <newline> <newline> <newline> The tribe ’ s life is a simple one , eking out a living from land and ruins , telling the stories and laughing , forgetting the wars that plagued a nation ’ s two thousand year past . The names of those who died are laughed about , and forgotten . The tribe is all that remains of a nation that once called itself *Gæraaathinia . * <newline> <newline> <newline> *Perhaps it is for the best . * <newline>\n",
      "\n",
      "LLM predicted human: Opening his eyes for the first time in what felt like an eternity, the man found himself in a dimly lit room, surrounded by walls that seemed to close in on him. His head throbbed with a dull ache as he tried to piece together memories that slipped through his grasp like sand through his fingers.\n",
      "\n",
      "As the fog in his mind began to clear, he realized he was alone; the silence of the room pressing down on him like a suffocating weight. With a gasp, he sat up, his hands trembling as he pushed himself to his feet.\n",
      "\n",
      "Where was he? How had he gotten here?\n",
      "\n",
      "Fragments of memories flashed before his eyes - a shadowy figure in the darkness, a piercing scream that echoed in his ears, and then... nothing.\n",
      "\n",
      "Fear gnawed at his insides, urging him to flee, but a deeper instinct told him to stay, to uncover the mysteries that lurked in the shadows of his mind.\n",
      "\n",
      "With a deep breath, he took a step forward, the floor creaking beneath his weight. Every fiber of his being screamed at him to turn back, but his curiosity drove him on.\n",
      "\n",
      "And so, the man ventured into the unknown, the first chapter of his story unfolding before him like a path through a dark and twisting forest, illuminated only by the flickering light of his wavering resolve.\n",
      "\n",
      "Prompt: 3\n",
      "Human predicted LLM: N/A\n",
      "LLM predicted human: In a quiet forest, a small deer looked around nervously, sensing the presence of danger lurking nearby. Suddenly, a rustle. Twigs crack. Predator emerges from shadows, eyes gleaming predatorily. Chase begins, heart pounding terror. Desperation mounts, breath labored. Escape. Sunset.\n",
      "\n",
      "Prompt: 4\n",
      "Human predicted LLM: The village ’ s harvest festival was fully underway , the sounds of children could be heard playing the carnival games , the smell of fried food engulfed the village inviting all in the valley to come join in their merriment , the invite however was also received by those who sought to sow the seeds of darkness throughout the lands . Warriors stood vigil at the entrance to the village at each gate , clad in their rusted armor they stood watching each visitor enter the village carefully inspecting each participant that passed them looking for signs that they had been corrupted . As the night began to dwindle and the light of the hearths dulled a darkness slowly swept over the village blotting out the ever increasing sunlight , the warriors drew their swords readying themselves for the battle that was to ensue . <newline> <newline> <newline> And so as the warriors stood at the precipice of change , their world was no longer their own and their feats consigned to legend . The rocks below fought back against the waves that crashed into them just as the warriors had over the course of the years , an always losing battle it was , but a battle they fought each and every day . They looked below then at each other before taking the plunge to the rocky shore below them , they would be the first ones to stem the tide of the spreading corruption and with their actions the light they fought so hard to protect was given new life as each one plunged . Their only hope would be that the mistakes that had been made would not be repeated again .\n",
      "\n",
      "LLM predicted human: First paragraph: \n",
      "The dusty old bookstore seemed out of place in the modern city, its weathered sign swinging gently in the breeze. Emma hesitated outside the door, wondering what secrets might be waiting inside.\n",
      "\n",
      "Last paragraph: \n",
      "As the sun dipped below the horizon, Emma emerged from the bookstore, clutching a worn leather-bound journal to her chest. A mysterious smile played on her lips, her eyes sparkling with a newfound sense of adventure. What she had discovered within those walls would change her life forever.\n",
      "\n",
      "Prompt: 5\n",
      "Human predicted LLM: N/A\n",
      "LLM predicted human: Dear [Recipient],\n",
      "\n",
      "As I sit down to write this long-awaited letter, I find myself flooded with a mix of emotions – relief, fear, and maybe even a hint of sadness. For so long, the words I am about to share have been locked away in the depths of my heart, waiting for the perfect moment to be set free.\n",
      "\n",
      "I've often wondered why I never found the courage to express myself to you before. Perhaps it was the fear of rejection or the worry of causing discomfort. But today, I choose to let go of those inhibitions and speak my truth.\n",
      "\n",
      "I want you to know that you have always held a special place in my life, even if I never had the opportunity to verbalize it. Your kindness, your laughter, and your mere presence have made a lasting impact on me, and for that, I am forever grateful.\n",
      "\n",
      "In some ways, I regret that I never mustered the courage to tell you how I truly feel. But at the same time, I believe that everything happens for a reason, and maybe this is the right moment for me to lay bare my soul.\n",
      "\n",
      "I wish you nothing but happiness, success, and love in all your endeavors. Know that you will always have a friend in me, even if our paths may diverge in the future.\n",
      "\n",
      "With love and gratitude,\n",
      "[Your Name]\n",
      "\n",
      "Prompt: 6\n",
      "Human predicted LLM: N/A\n",
      "LLM predicted human: In a bustling city where powers reign supreme and hierarchy is everything, you stand as the enigmatic and mysterious rank #1. Your power, obscured from the prying eyes and probing minds of your fellow citizens, is a closely guarded secret. While others flaunt their abilities openly and vie for supremacy, you quietly observe from the shadows, your true strength concealed from all.\n",
      "\n",
      "Perhaps it is this air of mystery that has allowed you to maintain your coveted rank at the top of the hierarchy. While others seek to climb the ladder through force and intimidation, you chart a different path, relying on cunning and strategy rather than raw power. Your presence is felt, yet your methods remain shrouded in secrecy.\n",
      "\n",
      "As the city buzzes with ambitions and rivalries, you navigate the treacherous terrain with a calm and calculated demeanor. Those who underestimate you do so at their peril, for behind your unassuming facade lies a formidable force that could tip the balance of power in an instant.\n",
      "\n",
      "So, as whispers of your enigmatic nature circulate through the city streets, the question remains: What hidden power do you wield that has allowed you to claim the coveted rank #1, and what mysteries lie in wait for those who dare to challenge you?\n",
      "\n",
      "Prompt: 7\n",
      "Human predicted LLM: She wanted to say so much . How he hurt her with his coldness . She wanted to say that each day as he grew more and more distant that it broke her . That deep inside part of her was dead . Not dead like a tree in winter that comes back to life with Spring . But real dead . Withered and twisted . She wanted to say that she missed the feeling of being the warmth in his life . She wanted to say that it had reached the point where she knew they could n't go on like this . That the only real chance they had now was to walk away , and try to start again . She wanted to say goodbye . <newline> <newline> Only she said she loved him\n",
      "\n",
      "LLM predicted human: She said she loved him, not realizing that he would be the one to ultimately break her heart.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:25<00:00, 25.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: hewlett\n",
      "Prompt: 0\n",
      "Human predicted LLM: N/A\n",
      "LLM predicted human: [Your Name]\n",
      "[Your Address]\n",
      "[City, State, Zip Code]\n",
      "[Email Address]\n",
      "[Phone Number]\n",
      "\n",
      "[Date]\n",
      "\n",
      "Editor,\n",
      "[Newspaper Name]\n",
      "[Address]\n",
      "[City, State, Zip Code]\n",
      "\n",
      "Dear Editor,\n",
      "\n",
      "I am writing to express my opinion on the effects computers have on people and to persuade your readers to consider a balanced perspective on this important issue.\n",
      "\n",
      "There is no denying that computers and technology have brought about numerous benefits to society. They have revolutionized the way we work, communicate, and learn. Computers teach hand-eye coordination, provide access to a vast amount of information, and allow people to connect with others from all corners of the globe. These advancements have undoubtedly enriched our lives in many ways.\n",
      "\n",
      "However, it is crucial to acknowledge the potential negative impact that excessive computer use can have on individuals and society as a whole. Spending too much time in front of a screen can lead to sedentary lifestyles, decreased physical activity, and social isolation. It is concerning to see individuals becoming more detached from real-life interactions and nature as they immerse themselves in the digital world.\n",
      "\n",
      "As a society, we must strive for a balance between the benefits of technology and the importance of maintaining our health, relationships, and connection to the natural world. Encouraging moderation in computer use, setting boundaries, and prioritizing activities that promote physical health and social well-being are essential steps in achieving this balance.\n",
      "\n",
      "In conclusion, while computers have undoubtedly brought about significant advancements and opportunities, it is essential to be mindful of their potential drawbacks. Let us embrace technology while also valuing time spent away from screens, engaging in physical activities, and nurturing meaningful connections with one another.\n",
      "\n",
      "Thank you for considering my perspective on this important issue.\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "[Your Name]\n",
      "\n",
      "Prompt: 1\n",
      "Human predicted LLM: N/A\n",
      "LLM predicted human: Title: Defending Free Expression in Libraries: A Case Against Censorship\n",
      "\n",
      "As a society that prides itself on freedom of expression and access to information, the issue of censorship in libraries is a contentious one. Some argue that certain materials should be removed from library shelves if found offensive or inappropriate. However, I firmly believe that censorship in libraries goes against the fundamental principles of intellectual freedom and should not be allowed.\n",
      "\n",
      "One of the main reasons why censorship in libraries should be opposed is that it undermines the diversity of thought and ideas that is essential in a democratic society. Libraries serve as important repositories of knowledge where individuals can explore a wide range of viewpoints and perspectives. By censoring certain materials, we risk stifling intellectual curiosity and limiting the exchange of ideas that is crucial for personal growth and societal progress.\n",
      "\n",
      "Furthermore, censorship in libraries sets a dangerous precedent by allowing individuals or groups to impose their own moral or ideological beliefs on others. What one person finds offensive or inappropriate may not be the same for someone else. By censoring materials based on subjective opinions, we run the risk of limiting access to valuable information and limiting the intellectual autonomy of library patrons.\n",
      "\n",
      "From a personal standpoint, I have always viewed libraries as sanctuaries of knowledge and learning, where individuals are free to explore and discover without judgment or restriction. One of my most cherished memories is stumbling upon a provocative book that challenged my beliefs and expanded my worldview. If that book had been censored or removed from the shelves, I would have missed out on a valuable opportunity for intellectual growth and self-discovery.\n",
      "\n",
      "In conclusion, censorship in libraries is a harmful practice that undermines the principles of intellectual freedom and limits access to diverse perspectives and ideas. Instead of censoring materials, libraries should strive to provide a wide range of resources that reflect the diversity of human experience. As Katherine Paterson so eloquently put it, if we start removing books from the shelves based on individual preferences, we risk losing the very essence of what makes libraries invaluable to society. Let us defend free expression in libraries and uphold the right of all individuals to access information without censorship.\n",
      "\n",
      "Prompt: 2\n",
      "Human predicted LLM: In the essay Do Not Exceed Posted Speed @CAPS1, the setting is used to alter the cyclists mood and as a reflection of his situation. For example, the statement the cool pines and rushing river of Yosemite had my name written all over them helps describe the cyclist situation. The idea of lush, cool water vegetation gives the @CAPS4 hope during his increasinlgly hellish trip. The beauty of that setting also reflects on the comfort that would come from reaching Yosemite; the good setting equal a good out come. Later on, the cyclist comes across one ramshackle shed, several rusts pump, and a corral that couldnt hold in the lamest make. This is a depressing setting symbolizing a complete loss of hope. And inteact at this point the cyclist becomes troubles to his situation. While pedaling through desert, the cyclist becomes even more factory when tumble seed, which is always a symbol of a deserted area, cones by. The nile of setting to affect and mirror mood is nothing new. There are many examples of this literary technique throughout history. For example, in his story The @CAPS2 of @CAPS3 @PERSON1. @CAPS4 @PERSON1 was a deslated land filled wild beast to symbolize and cause @PERSON2 to change his personalith surrounded by uncivilized people and creating @PERSON2 tears of his clothes and hunt like a dog. The setting of a desert in the uncultured. Part of colonial simultaneously cause and reflects on this change. The setting in the essay Do Not Exceed posted Speed Limit is used as an important for character development.  \n",
      "LLM predicted human: N/A\n",
      "\n",
      "Prompt: 3\n",
      "Human predicted LLM: In the story winter Hibiscus the author Minfong Ho ends with when they come back, Saeng vowed silently to herself, in the spring when the snows melt and geese return and this hibiscus is budding, then I will take that test again.\" she ended with this show her determination to do good in her country and show that memories of her grandmother help her to strive for the best in her new country, in her new home. In the beginning of the story Saeng is at a flower shop and she sees all the flowers of her home country growing in poto. At the sight of all these flowers memories flood back to her, memories that bring her joy and sadness. Then when she returns home she tells her mother that she failed the test you can tell from her actions that she is disappointed, I-I failed the test  Saeng did not dare look her mother in the eye  nervously tore off a leab,shredding it to bits. After a while she planted the flower in the vegetable garden. The buying and planting of the vegetable shows that Saeng wants to remember her grandmother and home. Thinking  about her grandmother helps her to survive for better, helps her want to succeed in her new home. Memories of her grandmother make her sad but in away make her happy.  \n",
      "LLM predicted human: N/A\n",
      "\n",
      "Prompt: 4\n",
      "Human predicted LLM: In the excerpt \"Narciso Rodriguez\" from the book Home: The Blueprints of Our Lives, the author tries to create a mood that complements his opinion on the definition of family. Narciso Rodriguez was born to Cuban immigrants in a busy area of New Jersey filled with people of all nationalities. It was this melting pot of people and his parents hard work that shaped who he is today.                                                                                                     The main mood that is generated throughout the passage is a positive mood, exemplified best by how the author himself positively talks about the great impact his parents had on him. Narciso realizes the sacrifices and the the hard work his parents had to endure to provide for him, of which Narciso writes \"My mother and father had come to this country with such courage, with no knowledge of the language or culture. They came selflessly, as many immigrants do, to give their children a better life, even if it meant leaving behind their families, friends, and careers in the country they loved.\" (paragraph @NUM1) Narciso's parents were not selfish at all and took a huge endeavor by going for the United States. However, through their hard work, determination, and couage they established themselves in @LOCATION3. It is this positive, happy outcome that makes the mood so bright and positive. In Narciso's home, as he writes, \"the innocence of childhood, the congregation of family and friends, and endless celebrations that encompassed both, formed the backdrop of life in our warm home.\" (paragraph @NUM2) The use of the word \"warm\" creates the bright vibrant mood associated with his home. The idea of warmth is repeated later in the passage where the author writes \"It was in the warmth of the kitchen in this humble house where a Cuban feast always filled the the air with not just scent but life and love\" (paragraph @NUM3) The repetition in describing the warmth of the house once again establishes the positive mood.                  Narciso lived in a neighborhood filled with all kinds of people. However, despite their differences they came together as a friendly, united community, of which the author write: \"Despite customs elsewhere, all of these cultures came together in great solidarity and friendship.\" (paragraph @NUM4) Despite the overt racism at the time, this community still came together and were there for each other no matter what. This unity with in the community definetely distills a happy mood within the reader. Overall, Narciso Rodriguez creates a very positive mood throughout the memoir.\n",
      "LLM predicted human: N/A\n",
      "\n",
      "Prompt: 5\n",
      "Human predicted LLM: Although high expectations were concieved for the Empire State Building, the builders faced many obstacles in attempting to allow dirigibles to dock there, as explained in the excerpt from The Mooring Mast by Marcia Amidon @CAPS1. The dream of having a mooring mast to dock new airships was quickly halted when architects first discovered the stress that a dirigible, held by a single cable tether, would add to the building's fame. As a result, \"the stress of the dirigible's load and the wind pressure would...be transmitted all the way to the building's foundation...nearly eleven hundred feet below\" (paragraph @NUM1). Consequently, architects had to reinforce the buildings framework to accommodate the tremendous amounts of stress. Secondly, the owners of the Empire State Building found that a law concerning airships became an obstacle as well. Stating that airships could not fly too low over populated urban areas, the law became another practical reason why dirigibles could not moor atop the miraculous building. Finally, architects had also discovered the greatest obstacle to their success to be nature itself. The winds atop the building were described to be violent, forceful and unpredictable. As a result, mooring dirigibles to the mast \"where they would be dangling high above pedestrians on the street, was neither practical nor safe\" (paragraph @NUM2). Moreover, the violent winds made it difficult for dirigibles to approach the mooring mast. In December 1930, the captain of the U.S. Navy dirigible \"Los Angeles\" feared that \"the wind would blow the dirigible into the sharp spires of other buildings in the area, which would puncture the dirigible's shell\" (paragraph @NUM3). In effect, the idea of using the mooring mast to dock dirigibles atop the Empire State Building quietly disappeared. As described by Marcia Amidon @CAPS1 in the excerpt from The Mooring Mast, the builders of the Empire State Building faced many obstacles in allowing dirigibles to dock there.\n",
      "LLM predicted human: The builders of the Empire State Building faced several obstacles when trying to allow dirigibles to dock at the building's mooring mast. One major obstacle was safety concerns related to the highly flammable nature of hydrogen gas used in dirigibles. The catastrophic destruction of the German dirigible Hindenburg in 1937 highlighted the potential danger of having a hydrogen-filled dirigible docked above a densely populated area like downtown New York. This safety risk was a significant barrier to the practical use of the mooring mast.\n",
      "\n",
      "Another obstacle was the challenging weather conditions present at the top of the Empire State Building. The excerpt mentions that the winds on top of the building were constantly shifting due to violent air currents. This made it difficult for dirigibles to remain stable and securely moored to the mast. The unpredictable nature of the wind currents posed a serious operational challenge for dirigibles attempting to dock at the building.\n",
      "\n",
      "Additionally, existing laws against airships flying too low over urban areas presented a practical barrier to the use of the mooring mast. These regulations made it illegal for dirigibles to tie up to the building or even approach the area. The legal restrictions on airships flying at low altitudes limited the ability of dirigibles to use the mooring mast as originally intended, further hindering the feasibility of the docking system.\n",
      "\n",
      "In summary, the obstacles faced by the builders of the Empire State Building in attempting to allow dirigibles to dock included safety concerns regarding hydrogen-filled dirigibles, challenging weather conditions at the building's height, and legal restrictions on airship operations over urban areas. These factors ultimately contributed to the unsuccessful implementation of the mooring mast for dirigibles at the Empire State Building.\n",
      "\n",
      "Prompt: 6\n",
      "Human predicted LLM: N/A\n",
      "LLM predicted human: Title: The Art of Patience\n",
      "\n",
      "Once upon a time, there lived a young girl named Lily who had always been known for her impatience. She couldn't stand waiting in lines, bore easily during long conversations, and often found herself frustrated when things didn't go her way immediately. Her impatience caused her unnecessary stress and frequently led to arguments with her friends and family.\n",
      "\n",
      "One day, Lily's grandmother gave her a special gift - a small plant in a pot. She told Lily that this plant would teach her the importance of patience. Lily rolled her eyes, thinking it was a strange gift, but she accepted it nonetheless.\n",
      "\n",
      "As days turned into weeks, Lily diligently watered the plant and watched as it began to grow. At first, she eagerly awaited for it to bloom, but as time went on, she realized that the plant was taking its own sweet time to grow. She started to understand that the plant needed time, care, and patience to fully blossom.\n",
      "\n",
      "Weeks turned into months, and Lily found herself becoming more patient as she tended to her plant. She no longer felt the need to rush things or get frustrated when things didn't happen as quickly as she wanted. She learned to appreciate the beauty of slow progress and the joy of witnessing growth over time.\n",
      "\n",
      "One day, as Lily walked by her plant, she noticed a small bud forming. She couldn't contain her excitement and eagerly waited for it to bloom. And when it finally did, she was overwhelmed with a sense of pride and fulfillment. She realized that the wait was worth it, and the beauty of patience was far more rewarding than instant gratification.\n",
      "\n",
      "From that day on, Lily's outlook on life changed. She became more understanding and tolerant, learning to navigate through challenges with grace and composure. Her friends and family noticed the transformation and admired her newfound patience.\n",
      "\n",
      "As the plant continued to thrive, so did Lily's patience. She understood that just like the plant, growth takes time, and the journey itself is as important as the destination. She embraced the art of patience, knowing that good things come to those who wait.\n",
      "\n",
      "And so, Lily's little plant not only blossomed into a beautiful flower but also blossomed her into a patient and resilient young woman, teaching her the valuable lesson that sometimes, the most beautiful things in life require patience to flourish.\n",
      "\n",
      "Prompt: 7\n",
      "Human predicted LLM: N/A\n",
      "LLM predicted human: A few years ago, my friend Emily and I decided to go on a camping trip to the mountains. It was our first time camping together, and we were both excited for the adventure ahead. As we set up our tent in the picturesque campsite surrounded by tall pine trees, we couldn't contain our enthusiasm.\n",
      "\n",
      "That night, after a delicious dinner cooked over the campfire, we sat around roasting marshmallows and sharing stories. The crackling fire and the peaceful ambiance of the forest set the perfect stage for our storytelling.\n",
      "\n",
      "At one point, Emily started sharing a funny childhood memory that had us both in stitches. She recounted a time when she mistook a neighbor's pet pig for a dog and ended up chasing it around the yard until the owner came out laughing. Her vivid storytelling and animated expressions had me laughing so hard that tears streamed down my face.\n",
      "\n",
      "The more we laughed, the lighter the atmosphere around us seemed. It was as if our bond grew stronger with each shared chuckle. Our laughter echoed through the trees, mixing with the sounds of the forest, creating a symphony of joy and camaraderie.\n",
      "\n",
      "As we curled up in our sleeping bags that night, I realized how much laughter had enhanced our camping experience. It not only brought us closer together as friends but also created unforgettable memories that we would cherish for years to come. Laughter truly was the magic ingredient that transformed a simple camping trip into a heartfelt connection between two people.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_trials = 10\n",
    "\n",
    "for dataset in [\"reddit\", \"hewlett\"]:\n",
    "    df_human = pd.read_csv(f\"{dataset}_responses/human.csv\")\n",
    "    df_human[\"author\"] = HUMAN\n",
    "    df_gpt_35 = pd.read_csv(f\"{dataset}_responses/{GPT_35}.csv\")\n",
    "    df_gpt_35[\"author\"] = GPT_35\n",
    "    df = pd.concat([df_human, df_gpt_35], ignore_index=True)\n",
    "\n",
    "    llm_predicted_human = {}\n",
    "    human_predicted_llm = {}\n",
    "\n",
    "    for _ in tqdm(range(n_trials)):\n",
    "        prompts = list(range(8))\n",
    "        random.shuffle(prompts)\n",
    "        test_prompts_list = [prompts[:2], prompts[2:4], prompts[4:6], prompts[6:8]]\n",
    "\n",
    "        for test_prompts in test_prompts_list:\n",
    "            df_test = classify(df, function_words=fw_oseah, test_prompts=test_prompts)\n",
    "\n",
    "            for prompt in test_prompts:\n",
    "                df_human_predicted_llm = df_test[\n",
    "                    (df_test[\"author\"] == HUMAN)\n",
    "                    & (df_test[\"y_pred\"] == GPT_35)\n",
    "                    & (df_test[\"prompt_id\"] == prompt)\n",
    "                ]\n",
    "                if not df_human_predicted_llm.empty:\n",
    "                    llm_predicted_human[prompt] = df_human_predicted_llm.iloc[0][\"text\"]\n",
    "\n",
    "                df_llm_predicted_human = df_test[\n",
    "                    (df_test[\"author\"] == GPT_35)\n",
    "                    & (df_test[\"y_pred\"] == HUMAN)\n",
    "                    & (df_test[\"prompt_id\"] == prompt)\n",
    "                ]\n",
    "                if not df_llm_predicted_human.empty:\n",
    "                    human_predicted_llm[prompt] = df_llm_predicted_human.iloc[0][\"text\"]\n",
    "\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for prompt in range(8):\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        if prompt in llm_predicted_human:\n",
    "            print(f\"Human predicted LLM: {llm_predicted_human[prompt]}\")\n",
    "        else:\n",
    "            print(\"Human predicted LLM: N/A\")\n",
    "        if prompt in human_predicted_llm:\n",
    "            print(f\"LLM predicted human: {human_predicted_llm[prompt]}\")\n",
    "        else:\n",
    "            print(\"LLM predicted human: N/A\")\n",
    "        print()\n",
    "    print(\"\\n\" * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 3.5 Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 35/42 [04:23<00:54,  7.81s/it]"
     ]
    }
   ],
   "source": [
    "n_trials = 10\n",
    "temps = [f\"{0.1 * i:.1f}\" for i in range(14)]\n",
    "\n",
    "for dataset in [\"reddit\", \"hewlett\"]:\n",
    "    results = []\n",
    "    for temp in tqdm(temps * n_trials):\n",
    "        df_human = pd.read_csv(f\"{dataset}_responses/human.csv\")\n",
    "        df_human[\"author\"] = HUMAN\n",
    "        df_temp = pd.read_csv(f\"{dataset}_responses/{GPT_35}_{temp}.csv\")\n",
    "        df_temp[\"author\"] = GPT_35\n",
    "        df = pd.concat([df_human, df_temp], ignore_index=True)\n",
    "\n",
    "        df_test = classify(df, fw_oseah)\n",
    "        report = classification_report(\n",
    "            df_test[\"author\"], df_test[\"y_pred\"], zero_division=0, output_dict=True\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"dataset\": dataset,\n",
    "                \"temp\": temp,\n",
    "                \"accuracy\": report[\"accuracy\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=results_df, x=\"temp\", y=\"accuracy\")\n",
    "    plt.title(\n",
    "        f\"Human vs. GPT-3.5 over a range of temperatures: {dataset.capitalize()} dataset\"\n",
    "    )\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Temperature\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for dataset in [\"reddit\", \"hewlett\"]:\n",
    "    dfs = []\n",
    "    for author in authors:\n",
    "        df = pd.read_csv(f\"{dataset}_responses/{author}.csv\")\n",
    "        df[\"author\"] = author\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    z_scores_transformer = ZScoreTransformer(function_words=fw_oseah)\n",
    "    z_scores = z_scores_transformer.fit_transform(df[\"text\"])\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(z_scores)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=df[\"author\"])\n",
    "    plt.title(f\"t-SNE Visualization of all prompts in {dataset.capitalize()} dataset\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "for dataset in [\"reddit\", \"hewlett\"]:\n",
    "    for prompt_id in range(8):\n",
    "        dfs = []\n",
    "        for author in authors:\n",
    "            df = pd.read_csv(f\"{dataset}_responses/{author}.csv\")\n",
    "            df[\"author\"] = author\n",
    "            dfs.append(df)\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        df = df[df[\"prompt_id\"] == prompt_id]\n",
    "\n",
    "        z_scores_transformer = ZScoreTransformer(function_words=fw_oseah)\n",
    "        z_scores = z_scores_transformer.fit_transform(df[\"text\"])\n",
    "\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(z_scores)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=df[\"author\"])\n",
    "        plt.title(\n",
    "            f\"t-SNE Visualization of prompt {prompt_id} in {dataset.capitalize()} dataset\"\n",
    "        )\n",
    "        plt.xlabel(\"t-SNE Dimension 1\")\n",
    "        plt.ylabel(\"t-SNE Dimension 2\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
