{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use min(8, 9) = 8 prompts, so both corpuses have the same number of prompts\n",
    "n_prompts = 8\n",
    "# We use the longest n_samples_per_prompt samples for each prompt, measured by number of words\n",
    "n_samples_per_prompt = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_0 = np.concatenate([[\"prompt_id\", \"text\"], [\"bert\"] * 768])\n",
    "index_1 = np.concatenate([[\"prompt_id\", \"text\"], [i for i in range(768)]])\n",
    "index = pd.MultiIndex.from_arrays([index_0, index_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for name in [\"train\", \"test\", \"valid\"]:\n",
    "    with open(\"reddit/writingPrompts/\" + name + \".wp_source\") as f_prompts:\n",
    "        prompts = f_prompts.readlines()\n",
    "    with open(\"reddit/writingPrompts/\" + name + \".wp_target\") as f_responses:\n",
    "        texts = f_responses.readlines()\n",
    "    assert len(prompts) == len(texts)\n",
    "    pairs.extend(list(zip(prompts, texts)))\n",
    "df_reddit = pd.DataFrame(pairs, columns=[\"prompt\", \"text\"])\n",
    "df_reddit = df_reddit[~df_reddit[\"prompt\"].str.contains(\"hitler\", case=False)]\n",
    "df_reddit = df_reddit.drop_duplicates(subset=[\"text\"])\n",
    "df_reddit = df_reddit[\n",
    "    df_reddit[\"prompt\"].isin(df_reddit[\"prompt\"].value_counts().index[:n_prompts])\n",
    "]\n",
    "df_reddit[\"prompt_id\"] = df_reddit[\"prompt\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Get the longest n_samples responses for each prompt\n",
    "df_reddit[\"text_len\"] = df_reddit[\"text\"].apply(lambda x: len(str.split(x)))\n",
    "df_reddit = (\n",
    "    df_reddit.groupby(\"prompt_id\")\n",
    "    .apply(lambda x: x.nlargest(n_samples_per_prompt, \"text_len\"), include_groups=False)\n",
    "    .reset_index(level=0, drop=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Break prompt into prompt and and prompt_tag\n",
    "df_reddit[\"prompt_tag\"] = (\n",
    "    df_reddit[\"prompt\"]\n",
    "    .str.split(\" \\]\", n=1)\n",
    "    .str[0]\n",
    "    .replace(\"\\[\", \"\", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "df_reddit[\"prompt\"] = df_reddit[\"prompt\"].str.split(\" \\]\", n=1).str[1].str.strip()\n",
    "df_reddit[\"prompt\"] = \"Prompt\\n\" + df_reddit[\"prompt\"]\n",
    "df_reddit[\"prompt_id\"] = df_reddit[\"prompt\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_prompts = (\n",
    "    df_reddit[[\"prompt_id\", \"prompt\", \"prompt_tag\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"prompt_id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "def modify_prompt(prompt):\n",
    "    prompt = prompt.split(\"\\n\")[1]\n",
    "    return \"Write a response to the following creative writing prompt:\\n\" + prompt\n",
    "\n",
    "\n",
    "df_reddit_prompts[\"prompt\"] = df_reddit_prompts[\"prompt\"].apply(modify_prompt)\n",
    "\n",
    "df_reddit_prompts.to_csv(\"reddit_prompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit = df_reddit[[\"prompt_id\", \"text\"]]\n",
    "df_reddit = df_reddit.sort_values(\"prompt_id\").reset_index(drop=True)\n",
    "df_reddit.to_csv(\"reddit_responses/human.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WP: Writing Prompt\n",
    "SP: Simple Prompt\n",
    "EU: Established Universe\n",
    "CW: Constrained Writing\n",
    "TT: Theme Thursday\n",
    "PM: Prompt Me\n",
    "MP: Media Prompt\n",
    "IP: Image Prompt\n",
    "PI: Prompt Inspired\n",
    "OT: Off Topic\n",
    "* OT as an Advertisement!\n",
    "RF: Reality Fiction\n",
    "```\n",
    "\n",
    "https://www.reddit.com/r/WritingPrompts/wiki/how_to_tag_prompts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Write a response to the following creative wri...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Write a response to the following creative wri...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Write a response to the following creative wri...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Write a response to the following creative wri...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Write a response to the following creative wri...</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Write a response to the following creative wri...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Write a response to the following creative wri...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Write a response to the following creative wri...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id                                             prompt prompt_tag\n",
       "0          0  Write a response to the following creative wri...         WP\n",
       "1          1  Write a response to the following creative wri...         WP\n",
       "2          2  Write a response to the following creative wri...         WP\n",
       "3          3  Write a response to the following creative wri...         WP\n",
       "4          4  Write a response to the following creative wri...         CW\n",
       "5          5  Write a response to the following creative wri...         WP\n",
       "6          6  Write a response to the following creative wri...         WP\n",
       "7          7  Write a response to the following creative wri...         WP"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>`` We left them there to study ! '' Proclaimed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>“ I suggest we initiate protocol Zestraol ” &lt;n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Our War Council was surprised when these Human...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>`` Drax , the Slovians have taken E13-49e , 4t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>`` They 've taken Marin , sir . '' &lt;newline&gt; &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>7</td>\n",
       "      <td>I 'd see her walking down the hall , her hair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>7</td>\n",
       "      <td>Wait . &lt;newline&gt; &lt;newline&gt; Doubt was settling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>7</td>\n",
       "      <td>It started as a chauvinistic affair meant to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>7</td>\n",
       "      <td>Izzard stalked through the once slicked stone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>7</td>\n",
       "      <td>He said he loved her and he wanted her , so th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id                                               text\n",
       "0            0  `` We left them there to study ! '' Proclaimed...\n",
       "1            0  “ I suggest we initiate protocol Zestraol ” <n...\n",
       "2            0  Our War Council was surprised when these Human...\n",
       "3            0  `` Drax , the Slovians have taken E13-49e , 4t...\n",
       "4            0  `` They 've taken Marin , sir . '' <newline> <...\n",
       "..         ...                                                ...\n",
       "795          7  I 'd see her walking down the hall , her hair ...\n",
       "796          7  Wait . <newline> <newline> Doubt was settling ...\n",
       "797          7  It started as a chauvinistic affair meant to m...\n",
       "798          7  Izzard stalked through the once slicked stone ...\n",
       "799          7  He said he loved her and he wanted her , so th...\n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the responses:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     800.00000\n",
       "mean      695.49625\n",
       "std       514.91980\n",
       "min       121.00000\n",
       "25%       290.00000\n",
       "50%       520.00000\n",
       "75%       932.25000\n",
       "max      2594.00000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words in the responses:\")\n",
    "df_reddit[\"text\"].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hewlett\n",
    "\n",
    "https://www.kaggle.com/competitions/asap-aes/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hewlett_prompts_dir = \"hewlett/prompts\"\n",
    "\n",
    "prompts = []\n",
    "for file in os.listdir(hewlett_prompts_dir):\n",
    "    with open(hewlett_prompts_dir + \"/\" + file) as f:\n",
    "        prompt = f.read()\n",
    "    prompts.append((int(file.split(\".\")[0]) - 1, prompt))\n",
    "\n",
    "df_hewlett_prompts = pd.DataFrame(prompts, columns=[\"prompt_id\", \"prompt\"])\n",
    "df_hewlett_prompts[\"prompt_tag\"] = df_hewlett_prompts[\"prompt\"].str.contains(\n",
    "    \"Source Essay\"\n",
    ")\n",
    "df_hewlett_prompts[\"prompt_tag\"] = df_hewlett_prompts[\"prompt_tag\"].replace(\n",
    "    {True: \"source dependent responses\", False: \"persuasive / narrative / expository\"}\n",
    ")\n",
    "df_hewlett_prompts = df_hewlett_prompts.sort_values(\"prompt_id\").reset_index(drop=True)\n",
    "df_hewlett_prompts.to_csv(\"hewlett_prompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hewlett_dir = \"hewlett\"\n",
    "\n",
    "filenames = [\n",
    "    \"training_set_rel3.tsv\",\n",
    "    \"valid_set.tsv\",\n",
    "    \"test_set.tsv\",\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(f\"{hewlett_dir}/{filename}\", sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "    df = df[[\"essay_set\", \"essay\"]]\n",
    "    df.rename(columns={\"essay_set\": \"prompt_id\", \"essay\": \"text\"}, inplace=True)\n",
    "    df[\"prompt_id\"] = df[\"prompt_id\"].astype(int).apply(lambda x: x - 1)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Don't need to remove the responses of any prompts because there are 8 distinct prompts in this dataset\n",
    "df_hewlett = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_hewlett = df_hewlett[df_hewlett[\"text\"] != \"\"]\n",
    "df_hewlett = df_hewlett.dropna()\n",
    "df_hewlett = df_hewlett.drop_duplicates()\n",
    "\n",
    "# Get the longest n_samples responses for each prompt\n",
    "df_hewlett[\"text_len\"] = df_hewlett[\"text\"].apply(lambda x: len(str.split(x)))\n",
    "df_hewlett = (\n",
    "    df_hewlett.groupby(\"prompt_id\")\n",
    "    .apply(lambda x: x.nlargest(n_samples_per_prompt, \"text_len\"), include_groups=False)\n",
    "    .reset_index(level=0, drop=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_hewlett = df_hewlett[[\"prompt_id\", \"text\"]]\n",
    "\n",
    "df_hewlett.to_csv(\"hewlett_responses/human.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prompt\\nMore and more people use computers, bu...</td>\n",
       "      <td>persuasive / narrative / expository</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Prompt\\nCensorship in the Libraries\\n\"All of u...</td>\n",
       "      <td>persuasive / narrative / expository</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Source Essay\\nROUGH ROAD AHEAD: Do Not Exceed ...</td>\n",
       "      <td>source dependent responses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Source Essay\\nWinter Hibiscus by Minfong Ho\\nS...</td>\n",
       "      <td>source dependent responses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Source Essay\\nNarciso Rodriguez\\nfrom Home: Th...</td>\n",
       "      <td>source dependent responses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Source Essay\\nThe Mooring Mast\\nby Marcia Amid...</td>\n",
       "      <td>source dependent responses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Prompt\\nWrite about patience. Being patient me...</td>\n",
       "      <td>persuasive / narrative / expository</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Prompt\\nWe all understand the benefits of laug...</td>\n",
       "      <td>persuasive / narrative / expository</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id                                             prompt  \\\n",
       "0          0  Prompt\\nMore and more people use computers, bu...   \n",
       "1          1  Prompt\\nCensorship in the Libraries\\n\"All of u...   \n",
       "2          2  Source Essay\\nROUGH ROAD AHEAD: Do Not Exceed ...   \n",
       "3          3  Source Essay\\nWinter Hibiscus by Minfong Ho\\nS...   \n",
       "4          4  Source Essay\\nNarciso Rodriguez\\nfrom Home: Th...   \n",
       "5          5  Source Essay\\nThe Mooring Mast\\nby Marcia Amid...   \n",
       "6          6  Prompt\\nWrite about patience. Being patient me...   \n",
       "7          7  Prompt\\nWe all understand the benefits of laug...   \n",
       "\n",
       "                            prompt_tag  \n",
       "0  persuasive / narrative / expository  \n",
       "1  persuasive / narrative / expository  \n",
       "2           source dependent responses  \n",
       "3           source dependent responses  \n",
       "4           source dependent responses  \n",
       "5           source dependent responses  \n",
       "6  persuasive / narrative / expository  \n",
       "7  persuasive / narrative / expository  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hewlett_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My standing postion on this cause is that comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@ORGANIZATION1, @CAPS1? Are you there?\" \"@CAPS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Dear The @CAPS1 newspaper, @CAPS2 in front of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Dear @CAPS1 Society: Computers are perhaps one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Dear @ORGANIZATION1, The creation of computers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>7</td>\n",
       "      <td>We couldn't control our selves, our eyes wate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>7</td>\n",
       "      <td>It all started at the play ground @CAPS9 me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>7</td>\n",
       "      <td>For my family laughter is important to us bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>7</td>\n",
       "      <td>Laughter, one of the greatest gifts in life. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>7</td>\n",
       "      <td>Why is it that people can look back at someth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id                                               text\n",
       "0            0  My standing postion on this cause is that comp...\n",
       "1            0  @ORGANIZATION1, @CAPS1? Are you there?\" \"@CAPS...\n",
       "2            0  Dear The @CAPS1 newspaper, @CAPS2 in front of ...\n",
       "3            0  Dear @CAPS1 Society: Computers are perhaps one...\n",
       "4            0  Dear @ORGANIZATION1, The creation of computers...\n",
       "..         ...                                                ...\n",
       "795          7   We couldn't control our selves, our eyes wate...\n",
       "796          7    It all started at the play ground @CAPS9 me ...\n",
       "797          7   For my family laughter is important to us bec...\n",
       "798          7   Laughter, one of the greatest gifts in life. ...\n",
       "799          7   Why is it that people can look back at someth...\n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hewlett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the responses:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     800.000000\n",
       "mean      465.585000\n",
       "std       246.627283\n",
       "min       205.000000\n",
       "25%       254.000000\n",
       "50%       351.500000\n",
       "75%       718.000000\n",
       "max      1064.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words in the responses:\")\n",
    "df_hewlett[\"text\"].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM response generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_responses(dataset_name, generate, model_name, temp=None):\n",
    "    prompts_df = pd.read_csv(f\"{dataset_name}_prompts.csv\")\n",
    "    prompt_ids = []\n",
    "    responses = []\n",
    "    for prompt, prompt_id in zip(prompts_df[\"prompt\"], prompts_df[\"prompt_id\"]):\n",
    "        for _ in tqdm(range(n_samples_per_prompt)):\n",
    "            prompt_ids.append(prompt_id)\n",
    "            if temp:\n",
    "                responses.append(generate(model_name, prompt, temp))\n",
    "            else:\n",
    "                responses.append(generate(model_name, prompt))\n",
    "    df = pd.DataFrame({\"prompt_id\": prompt_ids, \"text\": responses})\n",
    "    if temp:\n",
    "        df.to_csv(f\"{dataset_name}_responses/{model_name}_{temp}.csv\", index=False)\n",
    "    else:\n",
    "        df.to_csv(f\"{dataset_name}_responses/{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini (1.0 and 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/hstropkay/llm-style/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "\n",
    "with open(\"API_KEY_GOOGLE.txt\", \"r\") as f:\n",
    "    API_KEY_GOOGLE = f.read()\n",
    "\n",
    "genai.configure(api_key=API_KEY_GOOGLE)\n",
    "\n",
    "gemini_1 = \"gemini-1.0-pro\"\n",
    "gemini_15 = \"gemini-1.5-pro-latest\"\n",
    "\n",
    "\n",
    "def generate_gemini(model_name, prompt):\n",
    "    safety_settings = {\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }\n",
    "\n",
    "    model = genai.GenerativeModel(\n",
    "        f\"models/{model_name}\", safety_settings=safety_settings\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        start = time.time()\n",
    "        response = model.generate_content(prompt)\n",
    "        # Gemini sometimes returns an empty response due to \"SAFETY\", so try again\n",
    "        if not response.parts:\n",
    "            print(response.candidates)\n",
    "            continue\n",
    "        # Gemini has a rate limit of 15 requests per minute for 1.0 and 2 requests per minute for 1.5\n",
    "        wait_time = 30 if \"1.5\" in model_name else 4\n",
    "        time.sleep(max(0, wait_time + 1 - (time.time() - start)))\n",
    "        return \" \".join([part.text for part in response.parts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_responses(\"reddit\", generate_gemini, gemini_1)\n",
    "generate_responses(\"reddit\", generate_gemini, gemini_15)\n",
    "\n",
    "generate_responses(\"hewlett\", generate_gemini, gemini_1)\n",
    "generate_responses(\"hewlett\", generate_gemini, gemini_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT (3.5 and 4.0 and 3.5 over a range of temperature values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "with open(\"API_KEY_OPENAI.txt\", \"r\") as f:\n",
    "    API_KEY_OPENAI = f.read()\n",
    "\n",
    "model_name_35 = \"gpt-3.5-turbo-0125\"\n",
    "model_name_4 = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "\n",
    "def generate_gpt(model_name, prompt, temp=None):\n",
    "    client = OpenAI(api_key=API_KEY_OPENAI)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    response = None\n",
    "    if temp:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=temp,\n",
    "        )\n",
    "    else:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "        )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_responses(\"reddit\", generate_gpt, model_name_35)\n",
    "generate_responses(\"reddit\", generate_gpt, model_name_4)\n",
    "for temp in [i / 10 for i in range(16)]:\n",
    "    generate_responses(\"reddit\", generate_gpt, model_name_35, temp)\n",
    "\n",
    "generate_responses(\"hewlett\", generate_gpt, model_name_35)\n",
    "generate_responses(\"hewlett\", generate_gpt, model_name_4)\n",
    "for temp in [i / 10 for i in range(16)]:\n",
    "    generate_responses(\"hewlett\", generate_gpt, model_name_35, temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claude (Sonnet and Opus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "with open(\"API_KEY_ANTHROPIC.txt\", \"r\") as f:\n",
    "    API_KEY_ANTHROPIC = f.read()\n",
    "\n",
    "\n",
    "model_name_sonnet = \"claude-3-sonnet-20240229\"\n",
    "model_name_opus = \"claude-3-opus-20240229\"\n",
    "\n",
    "\n",
    "def generate_claude(model_name, prompt):\n",
    "    client = anthropic.Anthropic(api_key=API_KEY_ANTHROPIC)\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=model_name,\n",
    "        max_tokens=4096,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_responses(\"reddit\", generate_claude, model_name_sonnet)\n",
    "generate_responses(\"reddit\", generate_claude, model_name_opus)\n",
    "\n",
    "generate_responses(\"hewlett\", generate_claude, model_name_sonnet)\n",
    "generate_responses(\"hewlett\", generate_claude, model_name_opus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition replacement in the Hewlett dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Expand the bert column into 768 columns\n",
    "df_hewlett = pd.concat(\n",
    "    [df_hewlett, pd.DataFrame(np.zeros((len(df_hewlett), 768)))], axis=1\n",
    ")\n",
    "df_hewlett.columns = index\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
