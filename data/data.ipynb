{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use min(8, 9) = 8 prompts, so both corpuses have the same number of prompts\n",
    "n_prompts = 8\n",
    "# We use the longest n_samples_per_prompt samples for each prompt, measured by number of words\n",
    "n_samples_per_prompt = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_0 = np.concatenate([[\"prompt_id\", \"text\"], [\"bert\"] * 768])\n",
    "index_1 = np.concatenate([[\"prompt_id\", \"text\"], [i for i in range(768)]])\n",
    "index = pd.MultiIndex.from_arrays([index_0, index_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for name in [\"train\", \"test\", \"valid\"]:\n",
    "    with open(\"reddit/writingPrompts/\" + name + \".wp_source\") as f_prompts:\n",
    "        prompts = f_prompts.readlines()\n",
    "    with open(\"reddit/writingPrompts/\" + name + \".wp_target\") as f_responses:\n",
    "        texts = f_responses.readlines()\n",
    "    assert len(prompts) == len(texts)\n",
    "    pairs.extend(list(zip(prompts, texts)))\n",
    "df_reddit = pd.DataFrame(pairs, columns=[\"prompt\", \"text\"])\n",
    "df_reddit = df_reddit[~df_reddit[\"prompt\"].str.contains(\"hitler\", case=False)]\n",
    "df_reddit = df_reddit.drop_duplicates(subset=[\"text\"])\n",
    "df_reddit = df_reddit[\n",
    "    df_reddit[\"prompt\"].isin(df_reddit[\"prompt\"].value_counts().index[:n_prompts])\n",
    "]\n",
    "df_reddit[\"prompt_id\"] = df_reddit[\"prompt\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Get the longest n_samples responses for each prompt\n",
    "df_reddit[\"text_len\"] = df_reddit[\"text\"].apply(lambda x: len(str.split(x)))\n",
    "df_reddit = (\n",
    "    df_reddit.groupby(\"prompt_id\")\n",
    "    .apply(lambda x: x.nlargest(n_samples_per_prompt, \"text_len\"), include_groups=False)\n",
    "    .reset_index(level=0, drop=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Break prompt into prompt and and prompt_tag\n",
    "df_reddit[\"prompt_tag\"] = (\n",
    "    df_reddit[\"prompt\"]\n",
    "    .str.split(\" \\]\", n=1)\n",
    "    .str[0]\n",
    "    .replace(\"\\[\", \"\", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "df_reddit[\"prompt\"] = df_reddit[\"prompt\"].str.split(\" \\]\", n=1).str[1].str.strip()\n",
    "df_reddit[\"prompt\"] = \"Prompt\\n\" + df_reddit[\"prompt\"]\n",
    "df_reddit[\"prompt_id\"] = df_reddit[\"prompt\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_prompts = (\n",
    "    df_reddit[[\"prompt_id\", \"prompt\", \"prompt_tag\"]]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(\"prompt_id\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_reddit_prompts.to_csv(\"reddit_prompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit = df_reddit[[\"prompt_id\", \"text\"]]\n",
    "df_reddit = df_reddit.sort_values(\"prompt_id\").reset_index(drop=True)\n",
    "df_reddit.to_csv(\"reddit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WP: Writing Prompt\n",
    "SP: Simple Prompt\n",
    "EU: Established Universe\n",
    "CW: Constrained Writing\n",
    "TT: Theme Thursday\n",
    "PM: Prompt Me\n",
    "MP: Media Prompt\n",
    "IP: Image Prompt\n",
    "PI: Prompt Inspired\n",
    "OT: Off Topic\n",
    "* OT as an Advertisement!\n",
    "RF: Reality Fiction\n",
    "```\n",
    "\n",
    "https://www.reddit.com/r/WritingPrompts/wiki/how_to_tag_prompts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prompt\\nA peaceful alien race is besieged by a...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Prompt\\nThere is no prompt . Just write a stor...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Prompt\\nThis is the prologue ( or the first ch...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Prompt\\nWrite a short story where the first se...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Prompt\\nWrite the first and last paragraph of ...</td>\n",
       "      <td>CW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Prompt\\nWrite the letter that you always wante...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Prompt\\nYou live in a city full of people with...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Prompt\\n`` She said she loved him . '' Insert ...</td>\n",
       "      <td>WP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id                                             prompt prompt_tag\n",
       "0          0  Prompt\\nA peaceful alien race is besieged by a...         WP\n",
       "1          1  Prompt\\nThere is no prompt . Just write a stor...         WP\n",
       "2          2  Prompt\\nThis is the prologue ( or the first ch...         WP\n",
       "3          3  Prompt\\nWrite a short story where the first se...         WP\n",
       "4          4  Prompt\\nWrite the first and last paragraph of ...         CW\n",
       "5          5  Prompt\\nWrite the letter that you always wante...         WP\n",
       "6          6  Prompt\\nYou live in a city full of people with...         WP\n",
       "7          7  Prompt\\n`` She said she loved him . '' Insert ...         WP"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>`` We left them there to study ! '' Proclaimed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>“ I suggest we initiate protocol Zestraol ” &lt;n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Our War Council was surprised when these Human...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>`` Drax , the Slovians have taken E13-49e , 4t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>`` They 've taken Marin , sir . '' &lt;newline&gt; &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>7</td>\n",
       "      <td>I 'd see her walking down the hall , her hair ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>7</td>\n",
       "      <td>Wait . &lt;newline&gt; &lt;newline&gt; Doubt was settling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>7</td>\n",
       "      <td>It started as a chauvinistic affair meant to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>7</td>\n",
       "      <td>Izzard stalked through the once slicked stone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>7</td>\n",
       "      <td>He said he loved her and he wanted her , so th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id                                               text\n",
       "0            0  `` We left them there to study ! '' Proclaimed...\n",
       "1            0  “ I suggest we initiate protocol Zestraol ” <n...\n",
       "2            0  Our War Council was surprised when these Human...\n",
       "3            0  `` Drax , the Slovians have taken E13-49e , 4t...\n",
       "4            0  `` They 've taken Marin , sir . '' <newline> <...\n",
       "..         ...                                                ...\n",
       "795          7  I 'd see her walking down the hall , her hair ...\n",
       "796          7  Wait . <newline> <newline> Doubt was settling ...\n",
       "797          7  It started as a chauvinistic affair meant to m...\n",
       "798          7  Izzard stalked through the once slicked stone ...\n",
       "799          7  He said he loved her and he wanted her , so th...\n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the responses:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     800.00000\n",
       "mean      695.49625\n",
       "std       514.91980\n",
       "min       121.00000\n",
       "25%       290.00000\n",
       "50%       520.00000\n",
       "75%       932.25000\n",
       "max      2594.00000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words in the responses:\")\n",
    "df_reddit[\"text\"].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hewlett\n",
    "\n",
    "https://www.kaggle.com/competitions/asap-aes/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hewlett_prompts_dir = \"hewlett/prompts\"\n",
    "\n",
    "prompts = []\n",
    "for file in os.listdir(hewlett_prompts_dir):\n",
    "    with open(hewlett_prompts_dir + \"/\" + file) as f:\n",
    "        prompt = f.read()\n",
    "    prompts.append((int(file.split(\".\")[0]) - 1, prompt))\n",
    "\n",
    "df_hewlett_prompts = pd.DataFrame(prompts, columns=[\"prompt_id\", \"prompt\"])\n",
    "df_hewlett_prompts[\"prompt_tag\"] = df_hewlett_prompts[\"prompt\"].str.contains(\n",
    "    \"Source Essay\"\n",
    ")\n",
    "df_hewlett_prompts[\"prompt_tag\"] = df_hewlett_prompts[\"prompt_tag\"].replace(\n",
    "    {True: \"source dependent responses\", False: \"persuasive / narrative / expository\"}\n",
    ")\n",
    "df_hewlett_prompts = df_hewlett_prompts.sort_values(\"prompt_id\").reset_index(drop=True)\n",
    "df_hewlett_prompts.to_csv(\"hewlett_prompts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hewlett_dir = \"hewlett\"\n",
    "\n",
    "filenames = [\n",
    "    \"training_set_rel3.tsv\",\n",
    "    \"valid_set.tsv\",\n",
    "    \"test_set.tsv\",\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(f\"{hewlett_dir}/{filename}\", sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "    df = df[[\"essay_set\", \"essay\"]]\n",
    "    df.rename(columns={\"essay_set\": \"prompt_id\", \"essay\": \"text\"}, inplace=True)\n",
    "    df[\"prompt_id\"] = df[\"prompt_id\"].astype(int).apply(lambda x: x - 1)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Don't need to remove the responses of any prompts because there are 8 distinct prompts in this dataset\n",
    "df_hewlett = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "df_hewlett = df_hewlett[df_hewlett[\"text\"] != \"\"]\n",
    "df_hewlett = df_hewlett.dropna()\n",
    "df_hewlett = df_hewlett.drop_duplicates()\n",
    "\n",
    "# Get the longest n_samples responses for each prompt\n",
    "df_hewlett[\"text_len\"] = df_hewlett[\"text\"].apply(lambda x: len(str.split(x)))\n",
    "df_hewlett = (\n",
    "    df_hewlett.groupby(\"prompt_id\")\n",
    "    .apply(lambda x: x.nlargest(n_samples_per_prompt, \"text_len\"), include_groups=False)\n",
    "    .reset_index(level=0, drop=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_hewlett = df_hewlett[[\"prompt_id\", \"text\"]]\n",
    "\n",
    "df_hewlett.to_csv(\"hewlett.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>prompt_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prompt\\nMore and more people use computers, bu...</td>\n",
       "      <td>persuasive / narrative / expository</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Prompt\\nCensorship in the Libraries\\n\"All of u...</td>\n",
       "      <td>persuasive / narrative / expository</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Source Essay\\nROUGH ROAD AHEAD: Do Not Exceed ...</td>\n",
       "      <td>source dependent responses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Source Essay\\nWinter Hibiscus by Minfong Ho\\nS...</td>\n",
       "      <td>source dependent responses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Source Essay\\nNarciso Rodriguez\\nfrom Home: Th...</td>\n",
       "      <td>source dependent responses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Source Essay\\nThe Mooring Mast\\nby Marcia Amid...</td>\n",
       "      <td>source dependent responses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Prompt\\nWrite about patience. Being patient me...</td>\n",
       "      <td>persuasive / narrative / expository</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Prompt\\nWe all understand the benefits of laug...</td>\n",
       "      <td>persuasive / narrative / expository</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id                                             prompt  \\\n",
       "0          0  Prompt\\nMore and more people use computers, bu...   \n",
       "1          1  Prompt\\nCensorship in the Libraries\\n\"All of u...   \n",
       "2          2  Source Essay\\nROUGH ROAD AHEAD: Do Not Exceed ...   \n",
       "3          3  Source Essay\\nWinter Hibiscus by Minfong Ho\\nS...   \n",
       "4          4  Source Essay\\nNarciso Rodriguez\\nfrom Home: Th...   \n",
       "5          5  Source Essay\\nThe Mooring Mast\\nby Marcia Amid...   \n",
       "6          6  Prompt\\nWrite about patience. Being patient me...   \n",
       "7          7  Prompt\\nWe all understand the benefits of laug...   \n",
       "\n",
       "                            prompt_tag  \n",
       "0  persuasive / narrative / expository  \n",
       "1  persuasive / narrative / expository  \n",
       "2           source dependent responses  \n",
       "3           source dependent responses  \n",
       "4           source dependent responses  \n",
       "5           source dependent responses  \n",
       "6  persuasive / narrative / expository  \n",
       "7  persuasive / narrative / expository  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hewlett_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My standing postion on this cause is that comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@ORGANIZATION1, @CAPS1? Are you there?\" \"@CAPS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Dear The @CAPS1 newspaper, @CAPS2 in front of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Dear @CAPS1 Society: Computers are perhaps one...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Dear @ORGANIZATION1, The creation of computers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>7</td>\n",
       "      <td>We couldn't control our selves, our eyes wate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>7</td>\n",
       "      <td>It all started at the play ground @CAPS9 me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>7</td>\n",
       "      <td>For my family laughter is important to us bec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>7</td>\n",
       "      <td>Laughter, one of the greatest gifts in life. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>7</td>\n",
       "      <td>Why is it that people can look back at someth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id                                               text\n",
       "0            0  My standing postion on this cause is that comp...\n",
       "1            0  @ORGANIZATION1, @CAPS1? Are you there?\" \"@CAPS...\n",
       "2            0  Dear The @CAPS1 newspaper, @CAPS2 in front of ...\n",
       "3            0  Dear @CAPS1 Society: Computers are perhaps one...\n",
       "4            0  Dear @ORGANIZATION1, The creation of computers...\n",
       "..         ...                                                ...\n",
       "795          7   We couldn't control our selves, our eyes wate...\n",
       "796          7    It all started at the play ground @CAPS9 me ...\n",
       "797          7   For my family laughter is important to us bec...\n",
       "798          7   Laughter, one of the greatest gifts in life. ...\n",
       "799          7   Why is it that people can look back at someth...\n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hewlett"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the responses:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     800.000000\n",
       "mean      465.585000\n",
       "std       246.627283\n",
       "min       205.000000\n",
       "25%       254.000000\n",
       "50%       351.500000\n",
       "75%       718.000000\n",
       "max      1064.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of words in the responses:\")\n",
    "df_hewlett[\"text\"].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Expand the bert column into 768 columns\\ndf_hewlett = pd.concat(\\n    [df_hewlett, pd.DataFrame(np.zeros((len(df_hewlett), 768)))], axis=1\\n)\\ndf_hewlett.columns = index\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Expand the bert column into 768 columns\n",
    "df_hewlett = pd.concat(\n",
    "    [df_hewlett, pd.DataFrame(np.zeros((len(df_hewlett), 768)))], axis=1\n",
    ")\n",
    "df_hewlett.columns = index\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Laughter in life is a good thing to have I believe. Once @CAPS3 I was @NUM1 year's old I had a and @CAPS1. We always loved to hang out and do things together like take walks, or go to the beach, or go to the parks. What was so funny was that that we also loved to eat together too, maybe not out his dog bowl, but off of plates or on a picnic table. He was a cute dog. He had four little puppies named @CAPS5-@CAPS6, popcorn,big tail, and mountain. I named them all weird animal names because that's either what they smelled like or that's what they looked or loved doing.  Why we named my dog @CAPS1 was because he always smelt like @CAPS1. I always took him on walks to make him a healthy dog. It made me very angry @CAPS3 he had puppies and a mate. I don't know why maybe just because head I really loved my dog and i wanted him to be mine only. My favorite friend. I thought in my head hey! that's my dog and you leave him alone and do not touch him. Well @CAPS3 he had puppies i finally realized in my head @CAPS1 just wanted a life of his own too, a family, and be a big boy dog. He didn't hate me, he loved me and giving him a family is what I allowed and it made him happy. What was funny about the little puppies was that they smelt like odd food, like @CAPS1, @CAPS5 chops, or popcorn. My favorite puppy was @CAPS5 @CAPS6 because that's what he smelt like, not @CAPS1 but @CAPS5. POPCORN was a good dog too so was the other dogs but @CAPS5 @CAPS6 was a biter. It's funny because those little puppies love to bite and chew on stuff. The second you leave my puppies alone they will so eat everything. One time @CAPS3 we went on a trip to another state he had to leave the puppies alone with a babysitter, and @CAPS3 we came back they ate up a lot of the clothes and destroyed everything. I also had @CAPS11 too but they weren't good animals or pets to have they also ate everything up, and they stink. No not like @CAPS1 or popcorn but something else gross. The puppies loved to nibble on your toes and your hair. They had small teeth but oh they hurt. Stinky breath those dogs had I don't know why but there stinky. One of my favorite story about @CAPS1 was that he never liked to swim. It was odd to me because you would have thought cats hate water more than dogs. Well @CAPS1 did not know how to swim so I had to teach him myself as a good owner and all. Well @CAPS3 ever he doggy-paddled, he stuck his head underwater and swam forward. I don't know why, but I always thought can he breath like that? or is it just a doggy thing. Well eventually I got him over that, I don't think it was good for him to swim underwater. Like cats he used to never like water, it was just a phobia my dog had I guess. But I tried my best to get him over it. @CAPS3 I moved around states I had to leave him it was the saddest thing in my life ever. But we all move on right. Laughter I think is so good to have in your life because it can help you become more anti-depressed and not have to focus on the bad things in life. Every now and then @CAPS3 I am sad i just go and think of @CAPS1. I had other animals like @CAPS10, @CAPS11, @CAPS12, and cats. Personally I love cats more than dogs but @CAPS1 will always be the first. My cats weren't really that social to me, there lazy.What was so funny about my cats was that I named one of them @CAPS14. Why I named that cat @CAPS14 was because he was all black but with white feet. It was funny because @CAPS3 ever he got mad he did a lot of turning and jumping around. He was a wild kitty. I named one of my cats @CAPS15 because he was so fast at everything. What was funny about him was @CAPS3 ever you play around with him he charged off like he ate a bowl of cat nip. Having animals in my life to be with gave me a story of laughter in my life, and happiness. Laughter I think is a good important part of relationships too. If you do not laugh in your relationship then I believe that the couple are not so happy. Laughter can be good many time's, but also can be a insult too. If it means becoming a insult to some people or even laughing at people is rude.  Having laughter in your life is something you should always keep because it can help you become happy in many situations and problems you encounter in life. To be happy you should have laughter.\n",
      "\n",
      " It all began when my best guy friend @CAPS9 the time asked @CAPS7 to go to the lake that @DATE1 with a bunch of people , I mean how could I turn that down getting tan with friends, swimming I mean it sounded fun in all so I decided to say yes @CAPS5 really fast too. I was sitting on the couch when all the sudden my phone started ringing I hurried to pick it up.. it was @PERSON2 \" @PERSON1... @PERSON2 said \" \"yeah, whats up \" \"your coming right tomorrow right.. @PERSON2 continued \"\"yeah, of course what time though.\"  \"@CAPS2 going to be @CAPS9 your house @CAPS9 @NUM1 be ready @CAPS2 being @NUM2 exlaimed\" \" alright alright ,ill see you tomorrow.. @CAPS3 I hung up the phone.\"  The next day I layed in bed wide awake @CAPS9 @TIME1 @CAPS5 was unable to even think about sleeping any more I was so excited I had that feeling where I just knew it was going to be a good day @CAPS5 knew it was going to be so much fun. Nine o clock came faster @CAPS3 ever, I jumped out of bed @CAPS5 went straight to the bathroom to get ready, @CAPS5 begin packing my beach bag. @CAPS3 finally @NUM1 rolled around, @CAPS5 my phone started going off agin @CAPS5 it was @PERSON2 \" you ready!? were outside your house .. @PERSON2 said\" \"yep, @CAPS2 coming right now..bye \" I got in the car with it jamed full of guys @CAPS5 we took off.   @CAPS9 about two o clock we got to the lake @CAPS5 all began getting out of the car @CAPS5 started stretching @CAPS3 @CAPS3 all the boys started stripping off their clothes until just there swimming shorts @CAPS5 @CAPS3 all started running right towards the water @CAPS5 all hopped in. So right away I began taking my clothes off so now all I had on was my swimsuit @CAPS3 I stared walking down too. Finally @PERSON2 comes up @CAPS5 starts telling @CAPS7 to hurry \" @CAPS1'mon @PERSON1 hurry it up run I want you to get in...\" \" I know @CAPS2 coming.. @CAPS5 @CAPS2 going to get in dont worry @PERSON2.. \" @CAPS3 out of no where @PERSON2 grabes my hand @CAPS5 begins making @CAPS7 run down towards the water. When all the sudden we hit this like muddy patch of walk way area that looked like bird poop @CAPS5 mud mixed together. As soon as my feet hit the mud , while I was running I slipped @CAPS5 fell right in front of the guy I liked @CAPS9 the time \"@PERSON2\". I had never been more embarrassed in life, but I played it out cool, @CAPS5 since everyone else was laughing @CAPS9 @CAPS7 I decided to start laughing too. @CAPS3 I notcied that once everyone had stopped laughing no one really cared enough to make it that big of deal. So I hopped back up with the help of @PERSON2's hand @CAPS5 @CAPS3 notcied that since I had just slipped in mud/poop that it was all over the butt. @CAPS9 that point I dont think I could have been more embarrassed about what had just happened..I had slipped in mud, @CAPS5 it was all over @CAPS7. After that had happen I didnt really care anymore about being embarrassed, so I walked on down to the water @CAPS5 got in right away @CAPS5 swam around a little bit so that the mud would wash away, @CAPS5 thank @CAPS4 it did. About twenty mintues later were all sitting in the sand @CAPS5 just telling stories @CAPS5 hanging out, when @PERSON2 looks over in the water @CAPS5 notices that there is a huge \"water slug\" in the water right by him, so him being a guy, he decides to go over @CAPS5 grab the slug @CAPS5 hold on to it, @CAPS5 of course.. I freak out @CAPS5 get up @CAPS5 kind of take a few steps back. @PERSON2 starts asking \" @CAPS5 who do I want to throgh this @CAPS9\" everyone @CAPS9 the point is saying \" not @CAPS7.. NOT @CAPS7\". I just stood there quit, when he looks over @CAPS9 @CAPS7 I stare back @CAPS9 him with a scared look on my face @CAPS5 say \" @CAPS5 you wouldnt through that @CAPS9 @CAPS7 becauase @CAPS2 a good friend of yours right?.. now would you.\" @CAPS9 this point since right before this we were all sitting in the sand talking,I had put on a tank top @CAPS5 shorts over my swin suit, when all the sudden I hear \" @PERSON1 watch out\" I had turned around for a second... @CAPS9 the most, to look @CAPS9 something, @CAPS3 I feel this slimy long wet thing side down my shirt. I look down my shirt @CAPS5 see this slug in side of my shirt. I start freaking out @CAPS5 screaming as loud as I could @CAPS5 jumping around. @CAPS5 just when I thought it was out I look down @CAPS5 its still there finally it comes popping out of my shirt, @CAPS5 I just sat there calming down from what juast happend. Oh what a day!!\n",
      "\n",
      " Laughter is something that you should make an important part of your everyday life. In most cases laughter can brake the ice and can make people more comfortable with each other. When with friends and family most likely some of your best memory are made when there is something going on that made you laugh. Laughing not only puts yourself in a good mood but also every one around you. When you can make everyone laugh and put people in a good mood, in most cases you will be viewed as a fun, self confident person.  In the scorching @DATE1 my cousin @PERSON4, myself and two of his friends I had not met yet planned a camping trip. Our idea was that we are going down to the river about two miles away from @CAPS1 house and camp out for @NUM1 nights. We planed to make time the second day for us all to hike up the beautiful river to the falls for the day. When we first arrived i met is friends @PERSON2 and @PERSON3. We where all busy the first few hours setting up camp. Once camp was set up we made a fire and sat around it. The evening was awkward and silent for a short time because none of us other than @PERSON4 knew each other. Things started to get better when @PERSON4 started telling everyone about the prank war me and him had been having over the past few months. @CAPS2 started  \" this whole thing started when @PERSON1 put plastic wrap outside of my door so I ran in to it when i woke up in the morning.\" I interrupted. \" no actually this started because @PERSON4 called me short and made fun of me, I cant help that im not tall.\"  @CAPS2 continued his story about all the pranks we had pulled on each other like how i put butter all over his kitchen floor, then called his house phone so when @CAPS2 ran in there to answer it @CAPS2 slipped. @CAPS2 told of how @CAPS2 put his snake in my bed and scared me and how @CAPS2 filled my bathtub with green jello. After some good laughs we all warmed up and started talking and telling our own stories. Later that day we all went down to the river. @PERSON4 saw a small lime green frog and picked it up. @CAPS2 held it up to my face and said  \" kiss it @PERSON1 maybe it will turn in to a prince! \", I said \" no way dude im not going to kiss a frog\" @CAPS2 replied with \" and this is why you will never find true love @PERSON1 i mean you got to take a chance now and then you know.\"I told him that I had a prince and @CAPS2 was much more handsome then the frog could ever become.  After a good time at the river we went back up to camp and ate dinner and talked some more around the fire. That night @PERSON4 broke the zipper on his tent so lucky me I had to share my tent with him. After everyone went to bed we went out of the tent and where getting something to eat when @PERSON4 said \"whoa did you see that!\" as @CAPS2 quickly looked up at the sky. Naturally I looked up and said \"what is it.\" @CAPS2 just laughed at me because there was nothing there @CAPS2 just wanted me to look.  The next morning after breakfast me, @PERSON2, and @PERSON3 all went down to the river again and jumped in. @PERSON4 soon joined us. I noticed that there was a bunch of small fish in the water and i pointed them out to @PERSON4. The first thing @CAPS2 said was \"if you can catch one of those with your hands ill eat it hole\" I said \"your on\" so I struggled in the water trying to catch a little fish for around @NUM2 minutes. This whole time the guys are having fun laughing at me struggle and get all wet trying to catch the little fish. Finlay I got one! I walked over to @PERSON4 with the fish in my hand and smiled. @CAPS2 grabbed it out of my hand and put it in his mouth and swallow it, @CAPS2 quickly grabbed my hand and put it on his throat. I could feel the fish wiggling down his neck. I thought it was gross but then very funny because i didn't really expect him to do it.  The rest of the @ORGANIZATION1 consisted of some good laughs and some good memory's made. @PERSON2 somehow got coal from the fire on his forehead and we all giggled knowing @CAPS2 had no idea it was there for about @NUM1 hours. I caught @PERSON3 singing and dancing to himself by the river one evening, it was pretty hilarious but I kept that one to myself. Over all the camping trip was one of the best. I had a great time and made some new friends. It is truly an experience I will not soon forget.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = df_hewlett[df_hewlett[\"prompt_id\"] == 7].head(3)[\"text\"].to_list()\n",
    "\n",
    "for y in x:\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_prompt(prompt, dataset):\n",
    "    if dataset == \"reddit\":\n",
    "        prompt = prompt.split(\"\\n\")[1]\n",
    "        return (\n",
    "            \"Write a response the preceding creative writing prompt:\\n\" + prompt + \"\\n\"\n",
    "        )\n",
    "    else:\n",
    "        return prompt + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini (1.0 and 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/hstropkay/llm-style/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "\n",
    "\n",
    "with open(\"API_KEY_GOOGLE.txt\", \"r\") as f:\n",
    "    API_KEY_GOOGLE = f.read()\n",
    "\n",
    "genai.configure(api_key=API_KEY_GOOGLE)\n",
    "\n",
    "\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "gemini_10_pro = genai.GenerativeModel(\n",
    "    \"models/gemini-1.0-pro\", safety_settings=safety_settings\n",
    ")\n",
    "\n",
    "gemini_15_pro = genai.GenerativeModel(\n",
    "    \"models/gemini-1.5-pro-latest\", safety_settings=safety_settings\n",
    ")\n",
    "\n",
    "\n",
    "def generate_gemini(model, model_name, prompt):\n",
    "    while True:\n",
    "        start = time.time()\n",
    "        response = model.generate_content(prompt)\n",
    "        # Gemini sometimes returns an empty response due to \"SAFETY\", so try again\n",
    "        if not response.parts:\n",
    "            print(response.candidates)\n",
    "            continue\n",
    "        # Gemini has a rate limit of 15 requests per minute for 1.0 and 2 requests per minute for 1.5\n",
    "        wait_time = 30 if \"1.5\" in model_name else 4\n",
    "        time.sleep(max(0, wait_time + 1 - (time.time() - start)))\n",
    "        return \" \".join([part.text for part in response.parts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT (3.5 and 4.0 and 3.5 over a range of temperature values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m     API_KEY_OPENAI \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mAPI_KEY_OPENAI)\n\u001b[0;32m----> 8\u001b[0m completion \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCompose a poem that explains the concept of recursion in programming.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(completion\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/llm-style/.venv/lib/python3.10/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-style/.venv/lib/python3.10/site-packages/openai/resources/chat/completions.py:579\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 579\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-style/.venv/lib/python3.10/site-packages/openai/_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1231\u001b[0m     )\n\u001b[0;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/llm-style/.venv/lib/python3.10/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-style/.venv/lib/python3.10/site-packages/openai/_base_client.py:997\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    996\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/llm-style/.venv/lib/python3.10/site-packages/openai/_base_client.py:1045\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-style/.venv/lib/python3.10/site-packages/openai/_base_client.py:997\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    996\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/llm-style/.venv/lib/python3.10/site-packages/openai/_base_client.py:1045\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm-style/.venv/lib/python3.10/site-packages/openai/_base_client.py:1012\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1011\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1015\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1016\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1020\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "with open(\"API_KEY_OPENAI.txt\", \"r\") as f:\n",
    "    API_KEY_OPENAI = f.read()\n",
    "\n",
    "client = OpenAI(api_key=API_KEY_OPENAI)\n",
    "\n",
    "model_35 = \"gpt-3.5-turbo-0125\"\n",
    "model_4 = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_35,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Compose a poem that explains the concept of recursion in programming.\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)\n",
    "\n",
    "def generate_gpt(model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "r = []\n",
    "df_hewlett_prompts = pd.read_csv(\"hewlett_prompts.csv\")\n",
    "prompts = df_hewlett_prompts[\"prompt\"].to_list()\n",
    "for prompt in tqdm(prompts):\n",
    "    prompt = modify_prompt(prompt, \"hewlett\")\n",
    "    r.append(generate_gemini(gemini_15_pro, \"gemini-1.5-pro\", prompt))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "threads = []\n",
    "responses = np.zeros(len(prompts_800), dtype=object)\n",
    "for i, prompt in enumerate(tqdm(prompts_800)):\n",
    "    thread = threading.Thread(target=generate_response, args=(prompt, responses, i))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    time.sleep(2)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "responses = list(responses)\n",
    "df = pd.DataFrame({\"prompt\": prompts_800, \"response\": responses})\n",
    "df.to_csv(\"gemini1.csv\", index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
