{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogLocator\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"function_words/oshea.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    function_words = [line.split()[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN = \"human\"\n",
    "\n",
    "GEMINI_10 = \"gemini-1.0-pro\"\n",
    "GEMINI_15 = \"gemini-1.5-pro-latest\"\n",
    "\n",
    "CLAUDE_SONNET = \"claude-3-sonnet-20240229\"\n",
    "CLAUDE_OPUS = \"claude-3-opus-20240229\"\n",
    "\n",
    "GPT_35 = \"gpt-3.5-turbo-0125\"\n",
    "GPT_40 = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "LLMS = [GEMINI_10, GEMINI_15, CLAUDE_SONNET, CLAUDE_OPUS, GPT_35, GPT_40]\n",
    "AUTHORS = [HUMAN] + LLMS\n",
    "\n",
    "REDDIT = \"reddit\"\n",
    "HEWLETT = \"hewlett\"\n",
    "DATASETS = [REDDIT, HEWLETT]\n",
    "\n",
    "PAIRS = []\n",
    "for i, author1 in enumerate(AUTHORS):\n",
    "    for author2 in AUTHORS[i + 1 :]:\n",
    "        PAIRS.append((author1, author2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for dataset in DATASETS:\n",
    "    for author in AUTHORS:\n",
    "        df_cur = pd.read_csv(f\"{dataset}/responses/{author}.csv\")\n",
    "        df_cur[\"dataset\"] = dataset\n",
    "        df_cur[\"author\"] = author\n",
    "        df.append(df_cur)\n",
    "df = pd.concat(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZScoreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, function_words):\n",
    "        self.function_words = function_words\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            use_idf=False, norm=None, tokenizer=word_tokenize, token_pattern=None\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the vectorizer to the corpus\n",
    "        word_counts = self.vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "        # Save the function words and their indices if they are in the vocabulary\n",
    "        self.used_function_words = [\n",
    "            word for word in self.function_words if word in self.vectorizer.vocabulary_\n",
    "        ]\n",
    "        self.used_function_words_indices = [\n",
    "            self.vectorizer.vocabulary_[word] for word in self.used_function_words\n",
    "        ]\n",
    "\n",
    "        # Calculate the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Fit the z-score scaler\n",
    "        self.scaler.fit(relative_freqs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, z_score=True):\n",
    "        # Transform the corpus into word counts\n",
    "        word_counts = self.vectorizer.transform(X).toarray()\n",
    "\n",
    "        # Calculate the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        if not z_score:\n",
    "            return relative_freqs[:, self.used_function_words_indices]\n",
    "\n",
    "        # Calculate the z-scores\n",
    "        return self.scaler.transform(relative_freqs)[\n",
    "            :, self.used_function_words_indices\n",
    "        ]\n",
    "\n",
    "    def get_used_function_words(self):\n",
    "        return self.used_function_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA and t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "author_groups = PAIRS + [LLMS, AUTHORS]\n",
    "\n",
    "for reducer, reducer_name in [(PCA, \"PCA\"), (TSNE, \"t-SNE\")]:\n",
    "    for authors in author_groups:\n",
    "        if len(authors) == 2:\n",
    "            filename = f\"figures/{reducer_name}/{authors[0]}_{authors[1]}.png\"\n",
    "        elif len(authors) == 6:\n",
    "            filename = f\"figures/{reducer_name}/all_llms.png\"\n",
    "        else:\n",
    "            filename = f\"figures/{reducer_name}/all_authors.png\"\n",
    "\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "\n",
    "        df_sampled = (\n",
    "            df[(df[\"author\"].isin(authors))]\n",
    "            .groupby([\"dataset\", \"author\", \"prompt_id\"])\n",
    "            .apply(lambda x: x.sample(10), include_groups=False)\n",
    "            .reset_index(drop=False)\n",
    "        )\n",
    "\n",
    "        z_scores_transformer = ZScoreTransformer(function_words)\n",
    "        z_scores = z_scores_transformer.fit_transform(df_sampled[\"text\"])\n",
    "\n",
    "        dim_reducer = reducer(n_components=2)\n",
    "        z_scores_reduced = dim_reducer.fit_transform(z_scores)\n",
    "\n",
    "        df_reduced = pd.DataFrame(\n",
    "            z_scores_reduced, columns=[f\"{reducer_name} 1\", f\"{reducer_name} 2\"]\n",
    "        )\n",
    "        df_reduced[\"author\"] = df_sampled[\"author\"]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            data=df_reduced, x=f\"{reducer_name} 1\", y=f\"{reducer_name} 2\", hue=\"author\"\n",
    "        )\n",
    "        plt.title(f\"{reducer_name} over function word embeddings\")\n",
    "        plt.legend(title=\"Author\")\n",
    "        plt.savefig(filename)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(df, authors, function_words, return_df_coefs=False):\n",
    "    df = df[df[\"author\"].isin(authors)]\n",
    "\n",
    "    # n_responses_per_author_per_prompt_per_dataset = 10\n",
    "    # df = df.groupby([\"author\", \"prompt_id\", \"dataset\"]).sample(\n",
    "    #     n_responses_per_author_per_prompt_per_dataset\n",
    "    # )\n",
    "\n",
    "    # Train-test split: 12/4 (2 prompts from each dataset in the test set)\n",
    "    test_indices = []\n",
    "    for dataset in DATASETS:\n",
    "        test_prompts = np.random.choice(8, 2, replace=False)\n",
    "        test_indices.append(\n",
    "            (df[\"dataset\"] == dataset) & (df[\"prompt_id\"].isin(test_prompts))\n",
    "        )\n",
    "    test_indices = pd.concat(test_indices, axis=1).any(axis=1)\n",
    "\n",
    "    df_test = df[test_indices].copy()\n",
    "    df_train = df[~test_indices].copy()\n",
    "\n",
    "    # Set up 6-fold cross-validation\n",
    "    train_indices_by_prompt = list(\n",
    "        df_train.groupby([\"dataset\", \"prompt_id\"]).indices.values()\n",
    "    )\n",
    "\n",
    "    cv_iterable = []\n",
    "    for _ in range(6):\n",
    "        val_indices = np.concatenate(train_indices_by_prompt[:2])\n",
    "        train_indices = np.concatenate(train_indices_by_prompt[2:])\n",
    "        cv_iterable.append((train_indices, val_indices))\n",
    "        # Cycle indices list\n",
    "        train_indices_by_prompt = (\n",
    "            train_indices_by_prompt[2:] + train_indices_by_prompt[:2]\n",
    "        )\n",
    "\n",
    "    # Use the ZScoreTransformer to get the z-scores\n",
    "    z_scores_transformer = ZScoreTransformer(function_words)\n",
    "    z_scores_train = z_scores_transformer.fit_transform(df_train[\"text\"])\n",
    "    z_scores_test = z_scores_transformer.transform(df_test[\"text\"])\n",
    "\n",
    "    param_grid = {\n",
    "        \"C\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0],\n",
    "        \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    }\n",
    "\n",
    "    model = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_iterable,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "    )\n",
    "\n",
    "    # Model training and prediction\n",
    "    model.fit(z_scores_train, df_train[\"author\"])\n",
    "    df_test[\"author_pred\"] = model.predict(z_scores_test)\n",
    "\n",
    "    if not return_df_coefs:\n",
    "        return df_test\n",
    "\n",
    "    # Logistic regression model coefficients\n",
    "    coefs = model.best_estimator_.coef_.squeeze()\n",
    "\n",
    "    # For multiclass, return the average of the absolute values of the coefficients\n",
    "    if len(authors) > 2:\n",
    "        coefs = np.mean(np.abs(coefs), axis=0)\n",
    "\n",
    "    used_function_words = z_scores_transformer.get_used_function_words()\n",
    "    df_coefs = pd.DataFrame({\"word\": used_function_words, \"coef\": coefs})\n",
    "    return df_test, df_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from json import dumps\n",
    "\n",
    "N_TRIALS = 10\n",
    "\n",
    "\n",
    "def process_pair(pair_trial):\n",
    "    (author1, author2), trial = pair_trial\n",
    "    df_test, df_coefs = classify(\n",
    "        df=df,\n",
    "        authors=[author1, author2],\n",
    "        function_words=function_words,\n",
    "        return_df_coefs=True,\n",
    "    )\n",
    "\n",
    "    accuracy = sum(df_test[\"author\"] == df_test[\"author_pred\"]) / len(df_test)\n",
    "    df_test_json = dumps(df_test.to_json(orient=\"records\"))\n",
    "    df_coefs_json = dumps(df_coefs.to_json(orient=\"records\"))\n",
    "\n",
    "    df_results = {\n",
    "        \"author1\": author1,\n",
    "        \"author2\": author2,\n",
    "        \"trial\": trial,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"df_test\": df_test_json,\n",
    "        \"df_coefs\": df_coefs_json,\n",
    "    }\n",
    "    return df_results\n",
    "\n",
    "\n",
    "results_filename = \"classification_results/pairwise_classification.csv\"\n",
    "if os.path.exists(results_filename):\n",
    "    df_results = pd.read_csv(results_filename)\n",
    "else:\n",
    "    # Run in parallel using joblib\n",
    "    pairs_trials = list(product(PAIRS, range(N_TRIALS)))\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_pair)(pair_trial) for pair_trial in tqdm(pairs_trials)\n",
    "    )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(results_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_pivot = (\n",
    "    df_results[[\"author1\", \"author2\", \"accuracy\"]]\n",
    "    .groupby([\"author1\", \"author2\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot(index=\"author1\", columns=\"author2\", values=\"accuracy\")\n",
    "    .reindex(index=AUTHORS, columns=AUTHORS)\n",
    ")\n",
    "\n",
    "classification_figname = \"figures/pairwise/classification/heatmap.png\"\n",
    "if not os.path.exists(classification_figname):\n",
    "    sns.heatmap(bin_pivot, annot=True, vmin=0.5, vmax=1.0)\n",
    "    plt.title(\"Binary Logistic Regression Classification Accuracies\")\n",
    "    plt.ylabel(\"Author 1\")\n",
    "    plt.xlabel(\"Author 2\")\n",
    "    plt.savefig(classification_figname, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from io import StringIO\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df_cms = []\n",
    "for _, (author1, author2, trial, accuracy, df_test, df_coefs) in df_results.iterrows():\n",
    "    df_test = pd.read_json(StringIO(loads(df_test)))\n",
    "\n",
    "    cm = confusion_matrix(\n",
    "        df_test[\"author\"],\n",
    "        df_test[\"author_pred\"],\n",
    "        normalize=\"true\",\n",
    "        labels=[author1, author2],\n",
    "    )\n",
    "\n",
    "    zero_zero = cm[0, 0]\n",
    "    zero_one = cm[0, 1]\n",
    "    one_zero = cm[1, 0]\n",
    "    one_one = cm[1, 1]\n",
    "\n",
    "    df_cms.append(\n",
    "        {\n",
    "            \"author1\": author1,\n",
    "            \"author2\": author2,\n",
    "            \"zero_zero\": zero_zero,\n",
    "            \"zero_one\": zero_one,\n",
    "            \"one_zero\": one_zero,\n",
    "            \"one_one\": one_one,\n",
    "        }\n",
    "    )\n",
    "df_cms = pd.DataFrame(df_cms)\n",
    "df_cms = df_cms.groupby([\"author1\", \"author2\"]).mean().reset_index()\n",
    "\n",
    "for _, (author1, author2, z_z, z_o, o_z, o_o) in df_cms.iterrows():\n",
    "    classification_figname = (\n",
    "        f\"figures/pairwise/classification/confusion_matrices/{author1}_{author2}.png\"\n",
    "    )\n",
    "    if os.path.exists(classification_figname):\n",
    "        continue\n",
    "\n",
    "    authors = [author1, author2]\n",
    "    cm = np.array([[z_z, z_o], [o_z, o_o]])\n",
    "    cm = pd.DataFrame(cm, index=authors, columns=authors)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, vmin=0, vmax=1)\n",
    "    plt.title(\"Average Confusion Matrix for Binary Logistic Regression\")\n",
    "    plt.ylabel(\"True author\")\n",
    "    plt.xlabel(\"Predicted author\")\n",
    "    plt.savefig(classification_figname, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Humans and LLMs confused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM experiments ranked by the frequency of confusing the LLM for the human:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "author2\n",
       "claude-3-sonnet-20240229    0.25250\n",
       "claude-3-opus-20240229      0.21050\n",
       "gpt-3.5-turbo-0125          0.14800\n",
       "gemini-1.0-pro              0.08400\n",
       "gpt-4-turbo-2024-04-09      0.07775\n",
       "gemini-1.5-pro-latest       0.05475\n",
       "Name: one_zero, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_experiments = (\n",
    "    df_cms[df_cms[\"author1\"] == HUMAN].drop(columns=\"author1\").groupby(\"author2\").mean()\n",
    ")\n",
    "\n",
    "print(\"LLM experiments ranked by the frequency of confusing the LLM for the human:\")\n",
    "human_experiments[\"one_zero\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM experiments ranked by the frequency of confusing the human for the LLM:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "author2\n",
       "claude-3-opus-20240229      0.09900\n",
       "claude-3-sonnet-20240229    0.08775\n",
       "gpt-4-turbo-2024-04-09      0.06875\n",
       "gemini-1.5-pro-latest       0.06475\n",
       "gpt-3.5-turbo-0125          0.03375\n",
       "gemini-1.0-pro              0.01250\n",
       "Name: zero_one, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"LLM experiments ranked by the frequency of confusing the human for the LLM:\")\n",
    "human_experiments[\"zero_one\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average word coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from io import StringIO\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_coefs_list = []\n",
    "for i, (author1, author2, trial, accuracy, df_test, df_coefs) in df_results.iterrows():\n",
    "    df_coefs = pd.read_json(StringIO(loads(df_coefs)))\n",
    "    df_coefs[\"i\"] = i\n",
    "    df_coefs = df_coefs.pivot(index=\"i\", columns=\"word\", values=\"coef\")\n",
    "    df_coefs[\"author1\"] = author1\n",
    "    df_coefs[\"author2\"] = author2\n",
    "    df_coefs_list.append(df_coefs)\n",
    "\n",
    "df_coefs_list = pd.concat(df_coefs_list).replace(np.nan, 0)\n",
    "df_coefs_list = df_coefs_list.groupby([\"author1\", \"author2\"]).mean().abs()\n",
    "df_coefs_list = df_coefs_list.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "in             0.256229\n",
       "here           0.246231\n",
       "and            0.237781\n",
       "an             0.226464\n",
       "of             0.214341\n",
       "a              0.207266\n",
       "is             0.204516\n",
       "as             0.202178\n",
       "to             0.192250\n",
       "this           0.168080\n",
       "despite        0.162987\n",
       "which          0.162064\n",
       "with           0.158224\n",
       "that           0.149843\n",
       "moreover       0.149747\n",
       "was            0.148755\n",
       "not            0.144959\n",
       "may            0.141715\n",
       "furthermore    0.141356\n",
       "when           0.140737\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coefs_list.mean().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from json import dumps\n",
    "\n",
    "\n",
    "mc_results_filename = \"classification_results/multiclass_classification.csv\"\n",
    "\n",
    "if os.path.exists(mc_results_filename):\n",
    "    df_mc_results = pd.read_csv(mc_results_filename)\n",
    "else:\n",
    "    mc_results = []\n",
    "    for _ in tqdm(range(N_TRIALS)):\n",
    "        df_test, df_coefs = classify(\n",
    "            df=df,\n",
    "            authors=AUTHORS,\n",
    "            function_words=function_words,\n",
    "            return_df_coefs=True,\n",
    "        )\n",
    "\n",
    "        df_test_json = dumps(df_test.to_json(orient=\"records\"))\n",
    "        df_coefs_json = dumps(df_coefs.to_json(orient=\"records\"))\n",
    "\n",
    "        mc_results.append(\n",
    "            {\n",
    "                \"df_test\": df_test_json,\n",
    "                \"df_coefs\": df_coefs_json,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_mc_results = pd.DataFrame(mc_results)\n",
    "    df_mc_results.to_csv(mc_results_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "for _, (df_test, _) in df_mc_results.iterrows():\n",
    "    df_test = pd.read_json(StringIO(loads(df_test)))\n",
    "    cm = confusion_matrix(\n",
    "        df_test[\"author\"], df_test[\"author_pred\"], normalize=\"true\", labels=AUTHORS\n",
    "    )\n",
    "    cms.append(cm)\n",
    "multi_cm = np.mean(cms, axis=0)\n",
    "\n",
    "classification_figname = \"figures/multiclass/classification/confusion_matrix.png\"\n",
    "if not os.path.exists(classification_figname):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        multi_cm,\n",
    "        annot=True,\n",
    "        xticklabels=AUTHORS,\n",
    "        yticklabels=AUTHORS,\n",
    "        fmt=\".2f\",\n",
    "    )\n",
    "    plt.title(\"Multiclass Classification Confusion Matrix\")\n",
    "    plt.ylabel(\"True author\")\n",
    "    plt.xlabel(\"Predicted author\")\n",
    "    plt.savefig(classification_figname, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average word coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from io import StringIO\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_coefs_list = []\n",
    "for _, (_, df_coefs) in df_mc_results.iterrows():\n",
    "    df_coefs = pd.read_json(StringIO(loads(df_coefs)))\n",
    "    df_coefs[\"i\"] = i\n",
    "    df_coefs = df_coefs.pivot(index=\"i\", columns=\"word\", values=\"coef\")\n",
    "    df_coefs_list.append(df_coefs)\n",
    "\n",
    "df_coefs_list = pd.concat(df_coefs_list).replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "here       0.268697\n",
       "a          0.248612\n",
       "in         0.244271\n",
       "and        0.238381\n",
       "an         0.227587\n",
       "of         0.224629\n",
       "is         0.209168\n",
       "as         0.205005\n",
       "to         0.201880\n",
       "this       0.182059\n",
       "which      0.181930\n",
       "the        0.174612\n",
       "that       0.170505\n",
       "their      0.169423\n",
       "it         0.164873\n",
       "may        0.161354\n",
       "how        0.160643\n",
       "despite    0.159006\n",
       "when       0.158213\n",
       "not        0.154064\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_coef_mc = df_coefs_list.mean().sort_values(ascending=False).head(20)\n",
    "highest_coef_mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_figname = \"figures/word_frequencies/pronouns_heatmap.png\"\n",
    "pronouns = [\n",
    "    \"he\",\n",
    "    \"him\",\n",
    "    \"his\",\n",
    "    \"himself\",\n",
    "    \"she\",\n",
    "    \"her\",\n",
    "    \"herself\",\n",
    "    \"they\",\n",
    "    \"them\",\n",
    "    \"their\",\n",
    "    \"themselves\",\n",
    "]\n",
    "\n",
    "# Select the words with the highest coefficients in the multiclass classification\n",
    "high_coef_figname = \"figures/word_frequencies/high_coef_words_heatmap.png\"\n",
    "highest_coef_mc_words = list(highest_coef_mc.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_tuples = [\n",
    "    (\"Pronouns\", pronouns, pronoun_figname),\n",
    "    (\"High LR Coefficient Words\", highest_coef_mc_words, high_coef_figname),\n",
    "]\n",
    "\n",
    "for title, words, figname in word_list_tuples:\n",
    "    if os.path.exists(figname):\n",
    "        continue\n",
    "\n",
    "    all_word_frequencies = []\n",
    "    for word in words:\n",
    "        word_frequencies = {}\n",
    "        for author in AUTHORS:\n",
    "            author_df = df[df[\"author\"] == author]\n",
    "            word_counts = author_df[\"text\"].str.count(word)\n",
    "            word_freq = word_counts / author_df[\"text\"].str.split().apply(len)\n",
    "            word_frequencies[author] = word_freq.mean()\n",
    "        all_word_frequencies.append(word_frequencies)\n",
    "\n",
    "    frequencies_df = pd.DataFrame(all_word_frequencies, index=words)\n",
    "    frequencies_df = frequencies_df.div(frequencies_df[\"human\"], axis=0)\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    sns.heatmap(\n",
    "        frequencies_df.drop(columns=\"human\"),\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        norm=LogNorm(vmin=0.25, vmax=4),\n",
    "        cbar_kws={\"format\": \"%.2g\", \"ticks\": [0.25, 0.5, 1, 2, 4]},\n",
    "        cmap=sns.color_palette(\"vlag_r\", as_cmap=True),\n",
    "    )\n",
    "    plt.title(f\"LLM/Human Frequency Ratios over {title}\")\n",
    "    plt.xlabel(\"Author\")\n",
    "    plt.ylabel(\"Word\")\n",
    "    plt.savefig(figname, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dendrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. Binary classification\n",
    "# By turning the binary classification results into a similarity matrix\n",
    "np.fill_diagonal(bin_pivot.values, 1)\n",
    "binary_distance = 1 - bin_pivot.fillna(bin_pivot.T)\n",
    "\n",
    "# 2. Multiclass classification\n",
    "# By using the distribution of predictions in the multiclass LR as a feature vector for each author\n",
    "multiclass_distance = 1 - cosine_similarity(multi_cm)\n",
    "\n",
    "# 3. Average feature vector\n",
    "# By using the average feature vector from the z-score transformer for each author\n",
    "z_scores_transformer = ZScoreTransformer(function_words)\n",
    "z_scores = z_scores_transformer.fit_transform(df[\"text\"])\n",
    "z_scores_distance = 1 - cosine_similarity(\n",
    "    pd.DataFrame(z_scores, index=df[\"author\"])\n",
    "    .groupby(\"author\")\n",
    "    .mean()\n",
    "    .reindex(AUTHORS)\n",
    "    .values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # From https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "distance_matrices = {\n",
    "    \"Binary Classification\": binary_distance,\n",
    "    \"Multiclass Classification\": multiclass_distance,\n",
    "    \"Average Feature Vector\": z_scores_distance,\n",
    "}\n",
    "\n",
    "for title, distance_matrix in distance_matrices.items():\n",
    "    for linkage in [\"single\", \"average\", \"complete\"]:\n",
    "        figname = f\"figures/dendrograms/{title.replace(' ', '-').lower()}_{linkage}.png\"\n",
    "        if os.path.exists(figname):\n",
    "            continue\n",
    "\n",
    "        agg = AgglomerativeClustering(\n",
    "            n_clusters=1,\n",
    "            metric=\"precomputed\",\n",
    "            linkage=linkage,\n",
    "            compute_distances=True,\n",
    "        )\n",
    "\n",
    "        agg.fit(distance_matrix)\n",
    "\n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        plot_dendrogram(agg, labels=AUTHORS, leaf_font_size=10)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.savefig(figname, bbox_inches=\"tight\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnormalized = z_scores_transformer.fit(df[\"text\"]).transform(df[\"text\"], z_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the z-scores are the same as the StandardScaler on the unnormalized feature vectors\n",
    "assert np.max(StandardScaler().fit_transform(unnormalized) - z_scores) < 1e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance from the universal centroid of the average vector, average of element-wise (unnormalized):\n",
      "author\n",
      "human                       0.000714\n",
      "gpt-3.5-turbo-0125          0.000363\n",
      "gemini-1.5-pro-latest       0.000315\n",
      "gemini-1.0-pro              0.000313\n",
      "gpt-4-turbo-2024-04-09      0.000254\n",
      "claude-3-sonnet-20240229    0.000199\n",
      "claude-3-opus-20240229      0.000187\n",
      "dtype: float64 \n",
      "\n",
      "The Euclidean distance of the average from the universal centroid (unnormalized):\n",
      "author\n",
      "human                       0.028542\n",
      "gpt-3.5-turbo-0125          0.015391\n",
      "gemini-1.5-pro-latest       0.012507\n",
      "gemini-1.0-pro              0.011647\n",
      "gpt-4-turbo-2024-04-09      0.008786\n",
      "claude-3-opus-20240229      0.007653\n",
      "claude-3-sonnet-20240229    0.006135\n",
      "dtype: float64 \n",
      "\n",
      "The cosine distance between the average and the universal centroid (unnormalized):\n",
      "author\n",
      "human                       0.052774\n",
      "gemini-1.5-pro-latest       0.008931\n",
      "gpt-3.5-turbo-0125          0.006746\n",
      "gemini-1.0-pro              0.005987\n",
      "gpt-4-turbo-2024-04-09      0.005147\n",
      "claude-3-opus-20240229      0.003888\n",
      "claude-3-sonnet-20240229    0.002237\n",
      "dtype: float64 \n",
      "\n",
      "The distance from the universal centroid of the average vector, average of element-wise (z-scores):\n",
      "author\n",
      "human                       0.213904\n",
      "gemini-1.0-pro              0.108582\n",
      "gpt-3.5-turbo-0125          0.107867\n",
      "gemini-1.5-pro-latest       0.100212\n",
      "gpt-4-turbo-2024-04-09      0.095052\n",
      "claude-3-sonnet-20240229    0.078454\n",
      "claude-3-opus-20240229      0.064322\n",
      "dtype: float64 \n",
      "\n",
      "The Euclidean distance of the average from the universal centroid (z-scores):\n",
      "author\n",
      "human                       4.566826\n",
      "gpt-3.5-turbo-0125          2.349404\n",
      "gemini-1.0-pro              2.342910\n",
      "gemini-1.5-pro-latest       2.080608\n",
      "gpt-4-turbo-2024-04-09      2.010987\n",
      "claude-3-sonnet-20240229    1.859256\n",
      "claude-3-opus-20240229      1.372655\n",
      "dtype: float64 \n",
      "\n",
      "The cosine distance between the average and the universal centroid (z-scores):\n",
      "author\n",
      "gpt-3.5-turbo-0125          1.188693\n",
      "gemini-1.0-pro              1.162699\n",
      "gpt-4-turbo-2024-04-09      1.023045\n",
      "claude-3-opus-20240229      0.985346\n",
      "human                       0.926182\n",
      "claude-3-sonnet-20240229    0.888634\n",
      "gemini-1.5-pro-latest       0.852657\n",
      "dtype: float64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "unnormalized_df = pd.DataFrame(unnormalized, index=df[\"author\"])\n",
    "z_scores_df = pd.DataFrame(z_scores, index=df[\"author\"])\n",
    "\n",
    "df_dict = {\"unnormalized\": unnormalized_df, \"z-scores\": z_scores_df}\n",
    "\n",
    "for name, df_cur in df_dict.items():\n",
    "    universal_centroid = df_cur.mean()\n",
    "    grouped = df_cur.groupby(\"author\").mean()\n",
    "    authors = grouped.index\n",
    "\n",
    "    pw_distances = np.abs(grouped - universal_centroid)\n",
    "    pw_distances = pw_distances.mean(axis=1)\n",
    "    print(\n",
    "        f\"The distance from the universal centroid of the average vector, average of element-wise ({name}):\"\n",
    "    )\n",
    "    print(pw_distances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "    euclidian_distances = np.linalg.norm(grouped - universal_centroid, axis=1)\n",
    "    euclidian_distances = pd.Series(euclidian_distances, index=authors)\n",
    "    print(\n",
    "        f\"The Euclidean distance of the average from the universal centroid ({name}):\"\n",
    "    )\n",
    "    print(euclidian_distances.sort_values(ascending=False), \"\\n\")\n",
    "\n",
    "    # now for cosine distance\n",
    "    cosine_distances = []\n",
    "    for _, row in grouped.iterrows():\n",
    "        row = np.array(row).reshape(1, -1)\n",
    "        uc = np.array(universal_centroid).reshape(1, -1)\n",
    "        cosine_distances.append(1 - cosine_similarity(row, uc).item())\n",
    "    cosine_distances = pd.Series(cosine_distances, index=authors)\n",
    "    print(\n",
    "        f\"The cosine distance between the average and the universal centroid ({name}):\"\n",
    "    )\n",
    "    print(cosine_distances.sort_values(ascending=False), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18259485]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([[0, 1, 2, 10]], [[1, 25, 1, 2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
