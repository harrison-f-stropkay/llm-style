{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogLocator\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"function_words/oshea.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    function_words = [line.split()[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN = \"human\"\n",
    "\n",
    "GEMINI_10 = \"gemini-1.0-pro\"\n",
    "GEMINI_15 = \"gemini-1.5-pro-latest\"\n",
    "\n",
    "CLAUDE_SONNET = \"claude-3-sonnet-20240229\"\n",
    "CLAUDE_OPUS = \"claude-3-opus-20240229\"\n",
    "\n",
    "GPT_35 = \"gpt-3.5-turbo-0125\"\n",
    "GPT_40 = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "LLMS = [GEMINI_10, GEMINI_15, CLAUDE_SONNET, CLAUDE_OPUS, GPT_35, GPT_40]\n",
    "AUTHORS = [HUMAN] + LLMS\n",
    "\n",
    "REDDIT = \"reddit\"\n",
    "HEWLETT = \"hewlett\"\n",
    "DATASETS = [REDDIT, HEWLETT]\n",
    "\n",
    "PAIRS = []\n",
    "for i, author1 in enumerate(AUTHORS):\n",
    "    for author2 in AUTHORS[i + 1 :]:\n",
    "        PAIRS.append((author1, author2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for dataset in DATASETS:\n",
    "    for author in AUTHORS:\n",
    "        df_cur = pd.read_csv(f\"{dataset}/responses/{author}.csv\")\n",
    "        df_cur[\"dataset\"] = dataset\n",
    "        df_cur[\"author\"] = author\n",
    "        df.append(df_cur)\n",
    "df = pd.concat(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZScoreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, function_words):\n",
    "        self.function_words = function_words\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            use_idf=False, norm=None, tokenizer=word_tokenize, token_pattern=None\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the vectorizer to the corpus\n",
    "        word_counts = self.vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "        # Save the function words and their indices if they are in the vocabulary\n",
    "        self.used_function_words = [\n",
    "            word for word in self.function_words if word in self.vectorizer.vocabulary_\n",
    "        ]\n",
    "        self.used_function_words_indices = [\n",
    "            self.vectorizer.vocabulary_[word] for word in self.used_function_words\n",
    "        ]\n",
    "\n",
    "        # Fit z-score scaler to the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "        self.scaler.fit(relative_freqs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform the corpus into word counts\n",
    "        word_counts = self.vectorizer.transform(X).toarray()\n",
    "\n",
    "        # Calculate the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Calculate the z-scores\n",
    "        z_scores = self.scaler.transform(relative_freqs)[\n",
    "            :, self.used_function_words_indices\n",
    "        ]\n",
    "\n",
    "        return z_scores\n",
    "\n",
    "    def get_used_function_words(self):\n",
    "        return self.used_function_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for reducer, reducer_name in [(PCA, \"PCA\"), (TSNE, \"t-SNE\")]:\n",
    "    for author1, author2 in PAIRS:\n",
    "        filename = f\"figures/pairwise/{reducer_name}/{author1}_{author2}.png\"\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "\n",
    "        df_sampled = (\n",
    "            df[(df[\"author\"] == author1) | (df[\"author\"] == author2)]\n",
    "            .groupby([\"dataset\", \"author\", \"prompt_id\"])\n",
    "            .apply(lambda x: x.sample(10), include_groups=False)\n",
    "            .reset_index(drop=False)\n",
    "        )\n",
    "\n",
    "        z_scores_transformer = ZScoreTransformer(function_words)\n",
    "        z_scores = z_scores_transformer.fit_transform(df_sampled[\"text\"])\n",
    "\n",
    "        dim_reducer = reducer(n_components=2)\n",
    "        z_scores_reduced = dim_reducer.fit_transform(z_scores)\n",
    "\n",
    "        df_reduced = pd.DataFrame(\n",
    "            z_scores_reduced, columns=[f\"{reducer_name} 1\", f\"{reducer_name} 2\"]\n",
    "        )\n",
    "        df_reduced[\"author\"] = df_sampled[\"author\"]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            data=df_reduced, x=f\"{reducer_name} 1\", y=f\"{reducer_name} 2\", hue=\"author\"\n",
    "        )\n",
    "        plt.title(f\"{reducer_name} over function word embeddings\")\n",
    "        plt.legend(title=\"Author\")\n",
    "        plt.savefig(filename)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(df, authors, function_words, return_df_coefs=False):\n",
    "    df = df[df[\"author\"].isin(authors)]\n",
    "\n",
    "    # n_responses_per_author_per_prompt_per_dataset = 10\n",
    "    # df = df.groupby([\"author\", \"prompt_id\", \"dataset\"]).sample(\n",
    "    #     n_responses_per_author_per_prompt_per_dataset\n",
    "    # )\n",
    "\n",
    "    # Train-test split: 12/4 (2 prompts from each dataset in the test set)\n",
    "    test_indices = []\n",
    "    for dataset in DATASETS:\n",
    "        test_prompts = np.random.choice(8, 2, replace=False)\n",
    "        test_indices.append(\n",
    "            (df[\"dataset\"] == dataset) & (df[\"prompt_id\"].isin(test_prompts))\n",
    "        )\n",
    "    test_indices = pd.concat(test_indices, axis=1).any(axis=1)\n",
    "\n",
    "    df_test = df[test_indices].copy()\n",
    "    df_train = df[~test_indices].copy()\n",
    "\n",
    "    # Set up 6-fold cross-validation\n",
    "    train_indices_by_prompt = list(\n",
    "        df_train.groupby([\"dataset\", \"prompt_id\"]).indices.values()\n",
    "    )\n",
    "\n",
    "    cv_iterable = []\n",
    "    for _ in range(6):\n",
    "        val_indices = np.concatenate(train_indices_by_prompt[:2])\n",
    "        train_indices = np.concatenate(train_indices_by_prompt[2:])\n",
    "        cv_iterable.append((train_indices, val_indices))\n",
    "        # Cycle indices list\n",
    "        train_indices_by_prompt = (\n",
    "            train_indices_by_prompt[2:] + train_indices_by_prompt[:2]\n",
    "        )\n",
    "\n",
    "    # Use the ZScoreTransformer to get the z-scores\n",
    "    z_scores_transformer = ZScoreTransformer(function_words)\n",
    "    z_scores_train = z_scores_transformer.fit_transform(df_train[\"text\"])\n",
    "    z_scores_test = z_scores_transformer.transform(df_test[\"text\"])\n",
    "\n",
    "    param_grid = {\n",
    "        \"C\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0],\n",
    "        \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    }\n",
    "\n",
    "    model = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_iterable,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "    )\n",
    "\n",
    "    # Model training and prediction\n",
    "    model.fit(z_scores_train, df_train[\"author\"])\n",
    "    df_test[\"author_pred\"] = model.predict(z_scores_test)\n",
    "\n",
    "    if not return_df_coefs:\n",
    "        return df_test\n",
    "\n",
    "    # Logistic regression model coefficients\n",
    "    coefs = model.best_estimator_.coef_.squeeze()\n",
    "\n",
    "    # For multiclass, return the average of the absolute values of the coefficients\n",
    "    if len(authors) > 2:\n",
    "        coefs = np.mean(np.abs(coefs), axis=0)\n",
    "\n",
    "    used_function_words = z_scores_transformer.get_used_function_words()\n",
    "    df_coefs = pd.DataFrame({\"word\": used_function_words, \"coef\": coefs})\n",
    "    return df_test, df_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [01:01<00:00,  1.46s/it] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from json import dumps\n",
    "\n",
    "N_TRIALS = 2\n",
    "\n",
    "\n",
    "def process_pair(pair_trial):\n",
    "    (author1, author2), trial = pair_trial\n",
    "    df_test, df_coefs = classify(\n",
    "        df=df,\n",
    "        authors=[author1, author2],\n",
    "        function_words=function_words,\n",
    "        return_df_coefs=True,\n",
    "    )\n",
    "\n",
    "    accuracy = sum(df_test[\"author\"] == df_test[\"author_pred\"]) / len(df_test)\n",
    "    df_test_json = dumps(df_test.to_json(orient=\"records\"))\n",
    "    df_coefs_json = dumps(df_coefs.to_json(orient=\"records\"))\n",
    "\n",
    "    df_results = {\n",
    "        \"author1\": author1,\n",
    "        \"author2\": author2,\n",
    "        \"trial\": trial,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"df_test\": df_test_json,\n",
    "        \"df_coefs\": df_coefs_json,\n",
    "    }\n",
    "    return df_results\n",
    "\n",
    "\n",
    "results_filename = \"results/pairwise_classification.csv\"\n",
    "if os.path.exists(results_filename):\n",
    "    df_results = pd.read_csv(results_filename)\n",
    "else:\n",
    "    # Run in parallel using joblib\n",
    "    pairs_trials = list(product(PAIRS, range(N_TRIALS)))\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_pair)(pair_trial) for pair_trial in tqdm(pairs_trials)\n",
    "    )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(results_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = (\n",
    "    df_results[[\"author1\", \"author2\", \"accuracy\"]]\n",
    "    .groupby([\"author1\", \"author2\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot(index=\"author1\", columns=\"author2\", values=\"accuracy\")\n",
    "    .reindex(index=AUTHORS, columns=AUTHORS)\n",
    ")\n",
    "\n",
    "classification_figname = \"figures/pairwise/classification/heatmap.png\"\n",
    "if not os.path.exists(classification_figname):\n",
    "    sns.heatmap(pivot, annot=True, vmin=0.5, vmax=1.0)\n",
    "    plt.title(\"Binary Logistic Regression Classification Accuracies\")\n",
    "    plt.ylabel(\"Author 1\")\n",
    "    plt.xlabel(\"Author 2\")\n",
    "    plt.savefig(classification_figname, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from io import StringIO\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df_cms = []\n",
    "for _, (author1, author2, trial, accuracy, df_test, df_coefs) in df_results.iterrows():\n",
    "    df_test = pd.read_json(StringIO(loads(df_test)))\n",
    "\n",
    "    cm = confusion_matrix(df_test[\"author\"], df_test[\"author_pred\"], normalize=\"true\")\n",
    "\n",
    "    zero_zero = cm[0, 0]\n",
    "    zero_one = cm[0, 1]\n",
    "    one_zero = cm[1, 0]\n",
    "    one_one = cm[1, 1]\n",
    "\n",
    "    df_cms.append(\n",
    "        {\n",
    "            \"author1\": author1,\n",
    "            \"author2\": author2,\n",
    "            \"zero_zero\": zero_zero,\n",
    "            \"zero_one\": zero_one,\n",
    "            \"one_zero\": one_zero,\n",
    "            \"one_one\": one_one,\n",
    "        }\n",
    "    )\n",
    "df_cms = pd.DataFrame(df_cms)\n",
    "df_cms.groupby([\"author1\", \"author2\"]).mean().reset_index()\n",
    "\n",
    "for _, (author1, author2, z_z, z_o, o_z, o_o) in df_cms.iterrows():\n",
    "    authors = [author1, author2]\n",
    "    cm = np.array([[z_z, z_o], [o_z, o_o]])\n",
    "    cm = pd.DataFrame(cm, index=authors, columns=authors)\n",
    "\n",
    "    classification_figname = (\n",
    "        f\"figures/pairwise/classification/confusion_matrices/{author1}_{author2}.png\"\n",
    "    )\n",
    "    sns.heatmap(cm, annot=True, vmin=0, vmax=1)\n",
    "    plt.title(\"Average Confusion Matrix for Binary Logistic Regression\")\n",
    "    plt.ylabel(\"True author\")\n",
    "    plt.xlabel(\"Predicted author\")\n",
    "    plt.savefig(classification_figname, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average word coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "from io import StringIO\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_coefs_list = []\n",
    "for i, (author1, author2, trial, accuracy, df_test, df_coefs) in df_results.iterrows():\n",
    "    df_coefs = pd.read_json(StringIO(loads(df_coefs)))\n",
    "    df_coefs[\"i\"] = i\n",
    "    df_coefs = df_coefs.pivot(index=\"i\", columns=\"word\", values=\"coef\")\n",
    "    df_coefs[\"author1\"] = author1\n",
    "    df_coefs[\"author2\"] = author2\n",
    "    df_coefs_list.append(df_coefs)\n",
    "\n",
    "df_coefs_list = pd.concat(df_coefs_list).replace(np.nan, 0)\n",
    "df_coefs_list = df_coefs_list.groupby([\"author1\", \"author2\"]).mean().abs()\n",
    "df_coefs_list = df_coefs_list.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "in             0.254228\n",
       "here           0.238299\n",
       "an             0.229257\n",
       "and            0.225788\n",
       "a              0.214595\n",
       "of             0.214263\n",
       "as             0.209407\n",
       "to             0.204849\n",
       "is             0.173815\n",
       "moreover       0.158036\n",
       "this           0.156179\n",
       "that           0.155051\n",
       "with           0.153098\n",
       "not            0.150558\n",
       "furthermore    0.150114\n",
       "which          0.148979\n",
       "despite        0.148544\n",
       "how            0.142244\n",
       "it             0.140667\n",
       "when           0.139862\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coefs_list.mean().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average rank coef\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word\n",
       "and            39.333333\n",
       "of             42.809524\n",
       "a              42.857143\n",
       "as             43.095238\n",
       "in             49.476190\n",
       "this           50.285714\n",
       "with           51.238095\n",
       "an             53.952381\n",
       "is             55.809524\n",
       "despite        56.952381\n",
       "here           57.000000\n",
       "to             60.333333\n",
       "it             60.666667\n",
       "that           64.523810\n",
       "through        65.523810\n",
       "not            65.619048\n",
       "which          65.619048\n",
       "moreover       65.952381\n",
       "furthermore    69.761905\n",
       "he             71.190476\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_coefs_list.rank(axis=1, ascending=False).mean(axis=0).sort_values().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:27<00:00, 73.69s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from json import dumps\n",
    "\n",
    "\n",
    "mc_results_filename = \"results/multiclass_classification.csv\"\n",
    "\n",
    "if os.path.exists(mc_results_filename):\n",
    "    df_mc_results = pd.read_csv(mc_results_filename)\n",
    "else:\n",
    "    mc_results = []\n",
    "    for _ in tqdm(range(N_TRIALS)):\n",
    "        df_test, df_coefs = classify(\n",
    "            df=df,\n",
    "            authors=AUTHORS,\n",
    "            function_words=function_words,\n",
    "            return_df_coefs=True,\n",
    "        )\n",
    "\n",
    "        df_test_json = dumps(df_test.to_json(orient=\"records\"))\n",
    "        df_coefs_json = dumps(df_coefs.to_json(orient=\"records\"))\n",
    "\n",
    "        mc_results.append(\n",
    "            {\n",
    "                \"author1\": author1,\n",
    "                \"author2\": author2,\n",
    "                \"df_test\": df_test_json,\n",
    "                \"df_coefs\": df_coefs_json,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_mc_results = pd.DataFrame(mc_results)\n",
    "    df_mc_results.to_csv(mc_results_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_figname = \"figures/word_frequencies/pronouns_heatmap.png\"\n",
    "if not os.path.exists(pronoun_figname):\n",
    "    pronouns = [\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"himself\",\n",
    "        \"she\",\n",
    "        \"her\",\n",
    "        \"herself\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"themselves\",\n",
    "    ]\n",
    "\n",
    "    all_word_frequencies = []\n",
    "    for word in pronouns:\n",
    "        word_frequencies = {}\n",
    "        for author in AUTHORS:\n",
    "            author_df = df[df[\"author\"] == author]\n",
    "            word_counts = author_df[\"text\"].str.count(word)\n",
    "            word_freq = word_counts / author_df[\"text\"].str.split().apply(len)\n",
    "            word_frequencies[author] = word_freq.mean()\n",
    "        all_word_frequencies.append(word_frequencies)\n",
    "\n",
    "    frequencies_df = pd.DataFrame(all_word_frequencies, index=pronouns)\n",
    "    frequencies_df = frequencies_df.div(frequencies_df[\"human\"], axis=0)\n",
    "    sns.heatmap(\n",
    "        frequencies_df.drop(columns=\"human\"),\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        norm=LogNorm(vmin=0.25, vmax=4),\n",
    "        cbar_kws={\"format\": \"%.2g\", \"ticks\": [0.25, 0.5, 1, 2, 4]},\n",
    "        cmap=sns.color_palette(\"vlag_r\", as_cmap=True),\n",
    "    )\n",
    "    plt.title(f\"LLM/Human Pronoun Frequency Ratios\")\n",
    "    plt.xlabel(\"Author\")\n",
    "    plt.ylabel(\"Word\")\n",
    "    plt.savefig(pronoun_figname, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
