{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogLocator\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"function_words/oshea.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    function_words = [line.split()[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN = \"human\"\n",
    "\n",
    "GEMINI_10 = \"gemini-1.0-pro\"\n",
    "GEMINI_15 = \"gemini-1.5-pro-latest\"\n",
    "\n",
    "CLAUDE_SONNET = \"claude-3-sonnet-20240229\"\n",
    "CLAUDE_OPUS = \"claude-3-opus-20240229\"\n",
    "\n",
    "GPT_35 = \"gpt-3.5-turbo-0125\"\n",
    "GPT_40 = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "LLMS = [GEMINI_10, GEMINI_15, CLAUDE_SONNET, CLAUDE_OPUS, GPT_35, GPT_40]\n",
    "AUTHORS = [HUMAN] + LLMS\n",
    "\n",
    "REDDIT = \"reddit\"\n",
    "HEWLETT = \"hewlett\"\n",
    "DATASETS = [REDDIT, HEWLETT]\n",
    "\n",
    "PAIRS = []\n",
    "for i, author1 in enumerate(AUTHORS):\n",
    "    for author2 in AUTHORS[i + 1 :]:\n",
    "        PAIRS.append((author1, author2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for dataset in DATASETS:\n",
    "    for author in AUTHORS:\n",
    "        df_cur = pd.read_csv(f\"{dataset}/responses/{author}.csv\")\n",
    "        df_cur[\"dataset\"] = dataset\n",
    "        df_cur[\"author\"] = author\n",
    "        df.append(df_cur)\n",
    "df = pd.concat(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZScoreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, function_words):\n",
    "        self.function_words = function_words\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            use_idf=False, norm=None, tokenizer=word_tokenize, token_pattern=None\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the vectorizer to the corpus\n",
    "        word_counts = self.vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "        # Save the function words and their indices if they are in the vocabulary\n",
    "        self.used_function_words = [\n",
    "            word for word in self.function_words if word in self.vectorizer.vocabulary_\n",
    "        ]\n",
    "        self.used_function_words_indices = [\n",
    "            self.vectorizer.vocabulary_[word] for word in self.used_function_words\n",
    "        ]\n",
    "\n",
    "        # Fit z-score scaler to the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "        self.scaler.fit(relative_freqs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform the corpus into word counts\n",
    "        word_counts = self.vectorizer.transform(X).toarray()\n",
    "\n",
    "        # Calculate the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Calculate the z-scores\n",
    "        z_scores = self.scaler.transform(relative_freqs)[\n",
    "            :, self.used_function_words_indices\n",
    "        ]\n",
    "\n",
    "        return z_scores\n",
    "\n",
    "    def get_used_function_words(self):\n",
    "        return self.used_function_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for reducer, reducer_name in [(PCA, \"PCA\"), (TSNE, \"t-SNE\")]:\n",
    "    for author1, author2 in PAIRS:\n",
    "        filename = f\"figures/pairwise/{reducer_name}/{author1}_{author2}.png\"\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "\n",
    "        df_sampled = (\n",
    "            df[(df[\"author\"] == author1) | (df[\"author\"] == author2)]\n",
    "            .groupby([\"dataset\", \"author\", \"prompt_id\"])\n",
    "            .apply(lambda x: x.sample(10), include_groups=False)\n",
    "            .reset_index(drop=False)\n",
    "        )\n",
    "\n",
    "        z_scores_transformer = ZScoreTransformer(function_words)\n",
    "        z_scores = z_scores_transformer.fit_transform(df_sampled[\"text\"])\n",
    "\n",
    "        dim_reducer = reducer(n_components=2)\n",
    "        z_scores_reduced = dim_reducer.fit_transform(z_scores)\n",
    "\n",
    "        df_reduced = pd.DataFrame(\n",
    "            z_scores_reduced, columns=[f\"{reducer_name} 1\", f\"{reducer_name} 2\"]\n",
    "        )\n",
    "        df_reduced[\"author\"] = df_sampled[\"author\"]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            data=df_reduced, x=f\"{reducer_name} 1\", y=f\"{reducer_name} 2\", hue=\"author\"\n",
    "        )\n",
    "        plt.title(f\"{reducer_name} over function word embeddings\")\n",
    "        plt.legend(title=\"Author\")\n",
    "        plt.savefig(filename)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(df, authors, function_words, return_df_coefs=False):\n",
    "    df = df[df[\"author\"].isin(authors)]\n",
    "\n",
    "    # n_responses_per_author_per_prompt_per_dataset = 10\n",
    "    # df = df.groupby([\"author\", \"prompt_id\", \"dataset\"]).sample(\n",
    "    #     n_responses_per_author_per_prompt_per_dataset\n",
    "    # )\n",
    "\n",
    "    # Train-test split: 12/4 (2 prompts from each dataset in the test set)\n",
    "    test_indices = []\n",
    "    for dataset in DATASETS:\n",
    "        test_prompts = np.random.choice(8, 2, replace=False)\n",
    "        test_indices.append(\n",
    "            (df[\"dataset\"] == dataset) & (df[\"prompt_id\"].isin(test_prompts))\n",
    "        )\n",
    "    test_indices = pd.concat(test_indices, axis=1).any(axis=1)\n",
    "\n",
    "    df_test = df[test_indices].copy()\n",
    "    df_train = df[~test_indices].copy()\n",
    "\n",
    "    # Set up 6-fold cross-validation\n",
    "    train_indices_by_prompt = list(\n",
    "        df_train.groupby([\"dataset\", \"prompt_id\"]).indices.values()\n",
    "    )\n",
    "\n",
    "    cv_iterable = []\n",
    "    for _ in range(6):\n",
    "        val_indices = np.concatenate(train_indices_by_prompt[:2])\n",
    "        train_indices = np.concatenate(train_indices_by_prompt[2:])\n",
    "        cv_iterable.append((train_indices, val_indices))\n",
    "        # Cycle indices list\n",
    "        train_indices_by_prompt = (\n",
    "            train_indices_by_prompt[2:] + train_indices_by_prompt[:2]\n",
    "        )\n",
    "\n",
    "    # Use the ZScoreTransformer to get the z-scores\n",
    "    z_scores_transformer = ZScoreTransformer(function_words)\n",
    "    z_scores_train = z_scores_transformer.fit_transform(df_train[\"text\"])\n",
    "    z_scores_test = z_scores_transformer.transform(df_test[\"text\"])\n",
    "\n",
    "    param_grid = {\n",
    "        \"C\": [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0],\n",
    "        \"solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    }\n",
    "\n",
    "    model = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_iterable,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "    )\n",
    "\n",
    "    # Model training and prediction\n",
    "    model.fit(z_scores_train, df_train[\"author\"])\n",
    "    df_test[\"author_pred\"] = model.predict(z_scores_test)\n",
    "\n",
    "    if not return_df_coefs:\n",
    "        return df_test\n",
    "\n",
    "    # Logistic regression model coefficients\n",
    "    coefs = model.best_estimator_.coef_.squeeze()\n",
    "    used_function_words = z_scores_transformer.get_used_function_words()\n",
    "    df_coefs = pd.DataFrame({\"word\": used_function_words, \"coef\": coefs})\n",
    "    return df_test, df_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_TRIALS = 30\n",
    "\n",
    "\n",
    "def process_pair(pair_trial):\n",
    "    (author1, author2), trial = pair_trial\n",
    "    df_test = classify(\n",
    "        df=df,\n",
    "        authors=[author1, author2],\n",
    "        function_words=function_words,\n",
    "    )\n",
    "    accuracy = sum(df_test[\"author\"] == df_test[\"author_pred\"]) / len(df_test)\n",
    "    return {\n",
    "        \"author1\": author1,\n",
    "        \"author2\": author2,\n",
    "        \"trial\": trial,\n",
    "        \"accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "\n",
    "results_filename = \"results/pairwise_classification.csv\"\n",
    "if os.path.exists(results_filename):\n",
    "    df_results = pd.read_csv(results_filename)\n",
    "else:\n",
    "    # Run in parallel using joblib\n",
    "    pairs_trials = list(product(PAIRS, range(N_TRIALS)))\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(process_pair)(pair_trial) for pair_trial in tqdm(pairs_trials)\n",
    "    )\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(results_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = (\n",
    "    df_results.groupby([\"author1\", \"author2\"])\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .pivot(index=\"author1\", columns=\"author2\", values=\"accuracy\")\n",
    "    .reindex(index=AUTHORS, columns=AUTHORS)\n",
    ")\n",
    "\n",
    "classification_figname = \"figures/pairwise/classification/heatmap.png\"\n",
    "if not os.path.exists(classification_figname):\n",
    "    sns.heatmap(pivot, annot=True, vmin=0.5, vmax=1.0)\n",
    "    plt.title(\"Binary Logistic Regression Classification Accuracies\")\n",
    "    plt.ylabel(\"Author 1\")\n",
    "    plt.xlabel(\"Author 2\")\n",
    "    plt.savefig(classification_figname, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author1</th>\n",
       "      <th>author2</th>\n",
       "      <th>trial</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "      <td>0</td>\n",
       "      <td>0.96125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "      <td>1</td>\n",
       "      <td>0.97750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "      <td>2</td>\n",
       "      <td>0.97125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "      <td>3</td>\n",
       "      <td>0.98250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "      <td>4</td>\n",
       "      <td>0.95750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "      <td>25</td>\n",
       "      <td>0.89250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "      <td>26</td>\n",
       "      <td>0.76625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "      <td>27</td>\n",
       "      <td>0.85000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "      <td>28</td>\n",
       "      <td>0.82000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "      <td>29</td>\n",
       "      <td>0.83125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>630 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                author1                 author2  trial  accuracy\n",
       "0                 human          gemini-1.0-pro      0   0.96125\n",
       "1                 human          gemini-1.0-pro      1   0.97750\n",
       "2                 human          gemini-1.0-pro      2   0.97125\n",
       "3                 human          gemini-1.0-pro      3   0.98250\n",
       "4                 human          gemini-1.0-pro      4   0.95750\n",
       "..                  ...                     ...    ...       ...\n",
       "625  gpt-3.5-turbo-0125  gpt-4-turbo-2024-04-09     25   0.89250\n",
       "626  gpt-3.5-turbo-0125  gpt-4-turbo-2024-04-09     26   0.76625\n",
       "627  gpt-3.5-turbo-0125  gpt-4-turbo-2024-04-09     27   0.85000\n",
       "628  gpt-3.5-turbo-0125  gpt-4-turbo-2024-04-09     28   0.82000\n",
       "629  gpt-3.5-turbo-0125  gpt-4-turbo-2024-04-09     29   0.83125\n",
       "\n",
       "[630 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_figname = \"figures/frequencies/pronouns_heatmap.png\"\n",
    "if not os.path.exists(pronoun_figname):\n",
    "    pronouns = [\n",
    "        \"he\",\n",
    "        \"him\",\n",
    "        \"his\",\n",
    "        \"himself\",\n",
    "        \"she\",\n",
    "        \"her\",\n",
    "        \"herself\",\n",
    "        \"they\",\n",
    "        \"them\",\n",
    "        \"their\",\n",
    "        \"themselves\",\n",
    "    ]\n",
    "\n",
    "    all_word_frequencies = []\n",
    "    for word in pronouns:\n",
    "        word_frequencies = {}\n",
    "        for author in AUTHORS:\n",
    "            author_df = df[df[\"author\"] == author]\n",
    "            word_counts = author_df[\"text\"].str.count(word)\n",
    "            word_freq = word_counts / author_df[\"text\"].str.split().apply(len)\n",
    "            word_frequencies[author] = word_freq.mean()\n",
    "        all_word_frequencies.append(word_frequencies)\n",
    "\n",
    "    frequencies_df = pd.DataFrame(all_word_frequencies, index=pronouns)\n",
    "    frequencies_df = frequencies_df.div(frequencies_df[\"human\"], axis=0)\n",
    "    sns.heatmap(\n",
    "        frequencies_df.drop(columns=\"human\"),\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        norm=LogNorm(vmin=0.25, vmax=4),\n",
    "        cbar_kws={\"format\": \"%.2g\", \"ticks\": [0.25, 0.5, 1, 2, 4]},\n",
    "        cmap=sns.color_palette(\"vlag_r\", as_cmap=True),\n",
    "    )\n",
    "    plt.title(f\"LLM/Human Pronoun Frequency Ratios\")\n",
    "    plt.xlabel(\"Author\")\n",
    "    plt.ylabel(\"Word\")\n",
    "    plt.savefig(pronoun_figname, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
