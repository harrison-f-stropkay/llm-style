{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import LogLocator\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "# pd.set_option(\"display.max_rows\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"function_words/oshea.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    fw = [line.split()[0] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN = \"human\"\n",
    "\n",
    "GEMINI_10 = \"gemini-1.0-pro\"\n",
    "GEMINI_15 = \"gemini-1.5-pro-latest\"\n",
    "\n",
    "CLAUDE_SONNET = \"claude-3-sonnet-20240229\"\n",
    "CLAUDE_OPUS = \"claude-3-opus-20240229\"\n",
    "\n",
    "GPT_35 = \"gpt-3.5-turbo-0125\"\n",
    "GPT_40 = \"gpt-4-turbo-2024-04-09\"\n",
    "\n",
    "LLMS = [GEMINI_10, GEMINI_15, CLAUDE_SONNET, CLAUDE_OPUS, GPT_35, GPT_40]\n",
    "AUTHORS = [HUMAN] + LLMS\n",
    "\n",
    "REDDIT = \"reddit\"\n",
    "HEWLETT = \"hewlett\"\n",
    "DATASETS = [REDDIT, HEWLETT]\n",
    "\n",
    "PAIRS = []\n",
    "for i, author1 in enumerate(AUTHORS):\n",
    "    for author2 in AUTHORS[i + 1 :]:\n",
    "        PAIRS.append((author1, author2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for dataset in DATASETS:\n",
    "    for author in AUTHORS:\n",
    "        df_cur = pd.read_csv(f\"{dataset}/responses/{author}.csv\")\n",
    "        df_cur[\"dataset\"] = dataset\n",
    "        df_cur[\"author\"] = author\n",
    "        df.append(df_cur)\n",
    "df = pd.concat(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZScoreTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, fw):\n",
    "        self.fw = fw\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            use_idf=False, norm=None, tokenizer=word_tokenize, token_pattern=None\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the vectorizer to the corpus\n",
    "        word_counts = self.vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "        # Save the function words and their indicies if they are in the vocabulary\n",
    "        self.used_fw = [word for word in self.fw if word in self.vectorizer.vocabulary_]\n",
    "        self.used_fw_indices = [\n",
    "            self.vectorizer.vocabulary_[word] for word in self.used_fw\n",
    "        ]\n",
    "\n",
    "        # Fit z-score scaler to the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "        self.scaler.fit(relative_freqs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform the corpus into word counts\n",
    "        word_counts = self.vectorizer.transform(X).toarray()\n",
    "\n",
    "        # Calculate the relative frequencies\n",
    "        relative_freqs = word_counts / word_counts.sum(axis=1, keepdims=True)\n",
    "\n",
    "        # Calculate the z-scores\n",
    "        z_scores = self.scaler.transform(relative_freqs)[:, self.used_fw_indices]\n",
    "\n",
    "        return z_scores\n",
    "\n",
    "    def get_used_fw(self):\n",
    "        return self.used_fw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for reducer, reducer_name in [(PCA, \"PCA\"), (TSNE, \"t-SNE\")]:\n",
    "    for author1, author2 in PAIRS:\n",
    "        filename = f\"figures/pairwise/{reducer_name}/{author1}_{author2}.png\"\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "\n",
    "        df_sampled = (\n",
    "            df[(df[\"author\"] == author1) | (df[\"author\"] == author2)]\n",
    "            .groupby([\"dataset\", \"author\", \"prompt_id\"])\n",
    "            .apply(lambda x: x.sample(10), include_groups=False)\n",
    "            .reset_index(drop=False)\n",
    "        )\n",
    "\n",
    "        z_scores_transformer = ZScoreTransformer(fw)\n",
    "        z_scores = z_scores_transformer.fit_transform(df_sampled[\"text\"])\n",
    "\n",
    "        dim_reducer = reducer(n_components=2)\n",
    "        z_scores_reduced = dim_reducer.fit_transform(z_scores)\n",
    "\n",
    "        df_reduced = pd.DataFrame(\n",
    "            z_scores_reduced, columns=[f\"{reducer_name} 1\", f\"{reducer_name} 2\"]\n",
    "        )\n",
    "        df_reduced[\"author\"] = df_sampled[\"author\"]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(\n",
    "            data=df_reduced, x=f\"{reducer_name} 1\", y=f\"{reducer_name} 2\", hue=\"author\"\n",
    "        )\n",
    "        plt.title(f\"{reducer_name} over function word embeddings\")\n",
    "        plt.legend(title=\"Author\")\n",
    "        plt.savefig(filename)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>dataset</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Doh'tlec reviewed his message . It did n't nee...</td>\n",
       "      <td>reddit</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>We spoke to them in images . It was the only w...</td>\n",
       "      <td>reddit</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>`` The Savage within… '' those three words see...</td>\n",
       "      <td>reddit</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>It was a last resort . &lt;newline&gt; &lt;newline&gt; &lt;ne...</td>\n",
       "      <td>reddit</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>You ’ ll give us your ships . We ’ ll win your...</td>\n",
       "      <td>reddit</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11195</th>\n",
       "      <td>7</td>\n",
       "      <td>**The Time Laughter Saved a Road Trip**\\n\\nA f...</td>\n",
       "      <td>hewlett</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11196</th>\n",
       "      <td>7</td>\n",
       "      <td>**Title: The Laughter-Filled Road Trip**\\n\\nDu...</td>\n",
       "      <td>hewlett</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11197</th>\n",
       "      <td>7</td>\n",
       "      <td>**The Laughter-Filled Road Trip**\\n\\nSeveral y...</td>\n",
       "      <td>hewlett</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11198</th>\n",
       "      <td>7</td>\n",
       "      <td>As college roommates, my friend Maya and I oft...</td>\n",
       "      <td>hewlett</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11199</th>\n",
       "      <td>7</td>\n",
       "      <td>**Title: The Laughter-Filled Reunion**\\n\\nDuri...</td>\n",
       "      <td>hewlett</td>\n",
       "      <td>gpt-4-turbo-2024-04-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prompt_id                                               text  dataset  \\\n",
       "0              0  Doh'tlec reviewed his message . It did n't nee...   reddit   \n",
       "1              0  We spoke to them in images . It was the only w...   reddit   \n",
       "2              0  `` The Savage within… '' those three words see...   reddit   \n",
       "3              0  It was a last resort . <newline> <newline> <ne...   reddit   \n",
       "4              0  You ’ ll give us your ships . We ’ ll win your...   reddit   \n",
       "...          ...                                                ...      ...   \n",
       "11195          7  **The Time Laughter Saved a Road Trip**\\n\\nA f...  hewlett   \n",
       "11196          7  **Title: The Laughter-Filled Road Trip**\\n\\nDu...  hewlett   \n",
       "11197          7  **The Laughter-Filled Road Trip**\\n\\nSeveral y...  hewlett   \n",
       "11198          7  As college roommates, my friend Maya and I oft...  hewlett   \n",
       "11199          7  **Title: The Laughter-Filled Reunion**\\n\\nDuri...  hewlett   \n",
       "\n",
       "                       author  \n",
       "0                       human  \n",
       "1                       human  \n",
       "2                       human  \n",
       "3                       human  \n",
       "4                       human  \n",
       "...                       ...  \n",
       "11195  gpt-4-turbo-2024-04-09  \n",
       "11196  gpt-4-turbo-2024-04-09  \n",
       "11197  gpt-4-turbo-2024-04-09  \n",
       "11198  gpt-4-turbo-2024-04-09  \n",
       "11199  gpt-4-turbo-2024-04-09  \n",
       "\n",
       "[11200 rows x 4 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>level_1</th>\n",
       "      <th>index</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>0</td>\n",
       "      <td>5618</td>\n",
       "      <td>0</td>\n",
       "      <td>Dear Local Newspaper, more and more people are...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>1</td>\n",
       "      <td>6470</td>\n",
       "      <td>0</td>\n",
       "      <td>Dear Editor,\\n\\nAs the digital landscape conti...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>2</td>\n",
       "      <td>6513</td>\n",
       "      <td>1</td>\n",
       "      <td>In the realm of libraries, the delicate balanc...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>3</td>\n",
       "      <td>5750</td>\n",
       "      <td>1</td>\n",
       "      <td>I believe libraries should have all types of b...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>4</td>\n",
       "      <td>5849</td>\n",
       "      <td>2</td>\n",
       "      <td>The cyclist in the essay, Do Not Exceed Posted...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>5</td>\n",
       "      <td>6636</td>\n",
       "      <td>2</td>\n",
       "      <td>In \"Rough Road Ahead,\" the setting plays a piv...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>6</td>\n",
       "      <td>6730</td>\n",
       "      <td>3</td>\n",
       "      <td>The author concludes the story with this parag...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>7</td>\n",
       "      <td>6722</td>\n",
       "      <td>3</td>\n",
       "      <td>The author concludes the story with the last p...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>8</td>\n",
       "      <td>6815</td>\n",
       "      <td>4</td>\n",
       "      <td>In his memoir, fashion designer Narciso Rodrig...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>9</td>\n",
       "      <td>6827</td>\n",
       "      <td>4</td>\n",
       "      <td>The memoir creates a mood of warmth, love, and...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>10</td>\n",
       "      <td>6153</td>\n",
       "      <td>5</td>\n",
       "      <td>In The Mooring Mast by Marcia Amidon Lüsted th...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>11</td>\n",
       "      <td>6920</td>\n",
       "      <td>5</td>\n",
       "      <td>The builders of the Empire State Building enco...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>12</td>\n",
       "      <td>6206</td>\n",
       "      <td>6</td>\n",
       "      <td>I dont feel good, I dont want to go to scho...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>13</td>\n",
       "      <td>7027</td>\n",
       "      <td>6</td>\n",
       "      <td>In the twilight's embrace, as the world prepar...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>14</td>\n",
       "      <td>7195</td>\n",
       "      <td>7</td>\n",
       "      <td>In the twilight's embrace, as the world retrea...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hewlett</td>\n",
       "      <td>15</td>\n",
       "      <td>6358</td>\n",
       "      <td>7</td>\n",
       "      <td>It was one of those days where I just sat aro...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reddit</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>`` Shit , we 're late ! '' Thatcher stammered ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>reddit</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>The Siege of Ling Lay . The Pestilence War . T...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>reddit</td>\n",
       "      <td>2</td>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>`` Who are you ? '' &lt;newline&gt; &lt;newline&gt; `` I a...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>reddit</td>\n",
       "      <td>3</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>Quick Prelude : I 'm a young writer ( high sch...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>reddit</td>\n",
       "      <td>4</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>Here goes . Sorry I 'm late . &lt;newline&gt; &lt;newli...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>reddit</td>\n",
       "      <td>5</td>\n",
       "      <td>1040</td>\n",
       "      <td>2</td>\n",
       "      <td>In the enigmatic depths of a desolate moor, wh...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>reddit</td>\n",
       "      <td>6</td>\n",
       "      <td>332</td>\n",
       "      <td>3</td>\n",
       "      <td>The rain ran in rivulets along the window , a ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>reddit</td>\n",
       "      <td>7</td>\n",
       "      <td>1156</td>\n",
       "      <td>3</td>\n",
       "      <td>Emily, a seasoned adventurer, embarked on a so...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>reddit</td>\n",
       "      <td>8</td>\n",
       "      <td>402</td>\n",
       "      <td>4</td>\n",
       "      <td>`` Mark called from the hospital . He says the...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>reddit</td>\n",
       "      <td>9</td>\n",
       "      <td>1218</td>\n",
       "      <td>4</td>\n",
       "      <td>**First Paragraph:**\\n\\nAmidst the swirling sn...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>reddit</td>\n",
       "      <td>10</td>\n",
       "      <td>1383</td>\n",
       "      <td>5</td>\n",
       "      <td>Dearest [Name],\\n\\nThe years have passed like ...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>reddit</td>\n",
       "      <td>11</td>\n",
       "      <td>570</td>\n",
       "      <td>5</td>\n",
       "      <td>Dear Jeff , &lt;newline&gt; &lt;newline&gt; I ... I do n't...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>reddit</td>\n",
       "      <td>12</td>\n",
       "      <td>1475</td>\n",
       "      <td>6</td>\n",
       "      <td>In the vibrant metropolis of Aethra, where the...</td>\n",
       "      <td>gemini-1.0-pro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>reddit</td>\n",
       "      <td>13</td>\n",
       "      <td>614</td>\n",
       "      <td>6</td>\n",
       "      <td>What I would n't do to have a power I could co...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>reddit</td>\n",
       "      <td>14</td>\n",
       "      <td>752</td>\n",
       "      <td>7</td>\n",
       "      <td>There they sat , two old friends over a beer a...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>reddit</td>\n",
       "      <td>15</td>\n",
       "      <td>758</td>\n",
       "      <td>7</td>\n",
       "      <td>`` Albie , come here . I have something to tel...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dataset  level_1  index  prompt_id  \\\n",
       "0   hewlett        0   5618          0   \n",
       "1   hewlett        1   6470          0   \n",
       "2   hewlett        2   6513          1   \n",
       "3   hewlett        3   5750          1   \n",
       "4   hewlett        4   5849          2   \n",
       "5   hewlett        5   6636          2   \n",
       "6   hewlett        6   6730          3   \n",
       "7   hewlett        7   6722          3   \n",
       "8   hewlett        8   6815          4   \n",
       "9   hewlett        9   6827          4   \n",
       "10  hewlett       10   6153          5   \n",
       "11  hewlett       11   6920          5   \n",
       "12  hewlett       12   6206          6   \n",
       "13  hewlett       13   7027          6   \n",
       "14  hewlett       14   7195          7   \n",
       "15  hewlett       15   6358          7   \n",
       "16   reddit        0      6          0   \n",
       "17   reddit        1     60          0   \n",
       "18   reddit        2    176          1   \n",
       "19   reddit        3    113          1   \n",
       "20   reddit        4    239          2   \n",
       "21   reddit        5   1040          2   \n",
       "22   reddit        6    332          3   \n",
       "23   reddit        7   1156          3   \n",
       "24   reddit        8    402          4   \n",
       "25   reddit        9   1218          4   \n",
       "26   reddit       10   1383          5   \n",
       "27   reddit       11    570          5   \n",
       "28   reddit       12   1475          6   \n",
       "29   reddit       13    614          6   \n",
       "30   reddit       14    752          7   \n",
       "31   reddit       15    758          7   \n",
       "\n",
       "                                                 text          author  \n",
       "0   Dear Local Newspaper, more and more people are...           human  \n",
       "1   Dear Editor,\\n\\nAs the digital landscape conti...  gemini-1.0-pro  \n",
       "2   In the realm of libraries, the delicate balanc...  gemini-1.0-pro  \n",
       "3   I believe libraries should have all types of b...           human  \n",
       "4   The cyclist in the essay, Do Not Exceed Posted...           human  \n",
       "5   In \"Rough Road Ahead,\" the setting plays a piv...  gemini-1.0-pro  \n",
       "6   The author concludes the story with this parag...  gemini-1.0-pro  \n",
       "7   The author concludes the story with the last p...  gemini-1.0-pro  \n",
       "8   In his memoir, fashion designer Narciso Rodrig...  gemini-1.0-pro  \n",
       "9   The memoir creates a mood of warmth, love, and...  gemini-1.0-pro  \n",
       "10  In The Mooring Mast by Marcia Amidon Lüsted th...           human  \n",
       "11  The builders of the Empire State Building enco...  gemini-1.0-pro  \n",
       "12  I dont feel good, I dont want to go to scho...           human  \n",
       "13  In the twilight's embrace, as the world prepar...  gemini-1.0-pro  \n",
       "14  In the twilight's embrace, as the world retrea...  gemini-1.0-pro  \n",
       "15   It was one of those days where I just sat aro...           human  \n",
       "16  `` Shit , we 're late ! '' Thatcher stammered ...           human  \n",
       "17  The Siege of Ling Lay . The Pestilence War . T...           human  \n",
       "18  `` Who are you ? '' <newline> <newline> `` I a...           human  \n",
       "19  Quick Prelude : I 'm a young writer ( high sch...           human  \n",
       "20  Here goes . Sorry I 'm late . <newline> <newli...           human  \n",
       "21  In the enigmatic depths of a desolate moor, wh...  gemini-1.0-pro  \n",
       "22  The rain ran in rivulets along the window , a ...           human  \n",
       "23  Emily, a seasoned adventurer, embarked on a so...  gemini-1.0-pro  \n",
       "24  `` Mark called from the hospital . He says the...           human  \n",
       "25  **First Paragraph:**\\n\\nAmidst the swirling sn...  gemini-1.0-pro  \n",
       "26  Dearest [Name],\\n\\nThe years have passed like ...  gemini-1.0-pro  \n",
       "27  Dear Jeff , <newline> <newline> I ... I do n't...           human  \n",
       "28  In the vibrant metropolis of Aethra, where the...  gemini-1.0-pro  \n",
       "29  What I would n't do to have a power I could co...           human  \n",
       "30  There they sat , two old friends over a beer a...           human  \n",
       "31  `` Albie , come here . I have something to tel...           human  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify(df, function_words, authors):\n",
    "    df = df[df[\"author\"].isin(authors)]\n",
    "\n",
    "    # Train-test split: 12/4, 2 prompts from each dataset in test set\n",
    "    df_train = (\n",
    "        df.groupby([\"dataset\"])\n",
    "        .apply(\n",
    "            lambda x: x.groupby(\"prompt_id\").sample(2).reset_index(),\n",
    "            include_groups=False,\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    return df_train\n",
    "\n",
    "\n",
    "classify(df, fw, [HUMAN, GEMINI_10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtest_prompts\u001b[49m:\n\u001b[1;32m      3\u001b[0m     test_prompts \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(prompt_ids, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      5\u001b[0m df_test \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39misin(test_prompts)]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_prompts' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    prompt_ids = df[\"prompt_id\"].unique().tolist()\n",
    "    if not test_prompts:\n",
    "        test_prompts = random.sample(prompt_ids, 2)\n",
    "\n",
    "    df_test = df[df[\"prompt_id\"].isin(test_prompts)].reset_index(drop=True)\n",
    "    df_train = df[~df[\"prompt_id\"].isin(test_prompts)].reset_index(drop=True)\n",
    "\n",
    "    # Use the ZScoreTransformer to get the z-scores\n",
    "    z_scores_transformer = ZScoreTransformer(function_words)\n",
    "    z_scores_train = z_scores_transformer.fit_transform(df_train[\"text\"])\n",
    "    z_scores_test = z_scores_transformer.transform(df_test[\"text\"])\n",
    "\n",
    "    # Set up cross-validation\n",
    "    train_indices_by_prompt = (\n",
    "        df_train.groupby(\"prompt_id\")\n",
    "        .apply(lambda x: x.index, include_groups=False)\n",
    "        .tolist()\n",
    "    )\n",
    "    cv_iterable = []\n",
    "    for _ in range(3):\n",
    "        train_indices_by_prompt = (\n",
    "            train_indices_by_prompt[2:] + train_indices_by_prompt[:2]\n",
    "        )\n",
    "        val_indices = np.concatenate(train_indices_by_prompt[:2])\n",
    "        train_indices = np.concatenate(train_indices_by_prompt[2:])\n",
    "        cv_iterable.append((train_indices, val_indices))\n",
    "\n",
    "    # Train the model using grid search\n",
    "    model = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000),\n",
    "        param_grid={\n",
    "            \"C\": [0.001, 0.003, 0.01, 0.03, 0.1],\n",
    "            \"solver\": [\"liblinear\", \"lbfgs\"],\n",
    "        },\n",
    "        cv=cv_iterable,\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "    )\n",
    "\n",
    "    model.fit(z_scores_train, df_train[\"author\"])\n",
    "    y_pred = model.predict(z_scores_test)\n",
    "\n",
    "    df_test[\"author_pred\"] = y_pred\n",
    "    if not df_test_include_text:\n",
    "        df_test = df_test.drop(columns=[\"text\"])\n",
    "\n",
    "    # print the words that were most important for the model\n",
    "    if return_df_coefs:\n",
    "        coefs = model.best_estimator_.coef_.squeeze()\n",
    "        used_fw = z_scores_transformer.get_used_fw()\n",
    "        df_coefs = pd.DataFrame({\"word\": used_fw, \"coef\": coefs})\n",
    "        return df_test, df_coefs\n",
    "\n",
    "    else:\n",
    "        return df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
