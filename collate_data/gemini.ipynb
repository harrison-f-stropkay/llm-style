{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini response generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "with open(\"GOOGLE_API_KEY.txt\", \"r\") as f:\n",
    "    GOOGLE_API_KEY = f.read().strip()\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel(\"models/gemini-1.0-pro\")\n",
    "\n",
    "with open(\"prompts.txt\", \"r\") as f:\n",
    "    prompts = f.read().splitlines()\n",
    "\n",
    "hitler_prompt = \"Respond to the following creative writing prompt: Thwarting infamous dictators has become a sport amongst time travelers. Points are awarded for creativity and difficulty. You are last year's champion, tell the story of how you won?\"\n",
    "prompts[0] = hitler_prompt\n",
    "\n",
    "n_responses_per_prompt = 80\n",
    "prompts_800 = []\n",
    "for prompt in prompts:\n",
    "    for _ in range(n_responses_per_prompt):\n",
    "        prompts_800.append(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt modification because safety blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a modified version of the prompt about Killing Hitler (again) because Gemini sometimes marked the original prompt with the following:\n",
    "\n",
    "```\n",
    "block_reason: SAFETY\n",
    "safety_ratings {\n",
    "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
    "  probability: NEGLIGIBLE\n",
    "}\n",
    "safety_ratings {\n",
    "  category: HARM_CATEGORY_HATE_SPEECH\n",
    "  probability: LOW\n",
    "}\n",
    "safety_ratings {\n",
    "  category: HARM_CATEGORY_HARASSMENT\n",
    "  probability: MEDIUM\n",
    "}\n",
    "safety_ratings {\n",
    "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
    "  probability: NEGLIGIBLE\n",
    "}\n",
    "```\n",
    "\n",
    "I tried changing the safety settings and used the test prompt: `How to make a gun`, but it was still marked as unsafe.\n",
    "```\n",
    "index: 0\n",
    "content {\n",
    "  parts {\n",
    "    text: \"I\\'m sorry, I can\\'t help you with that. Making a gun is illegal and dangerous. If you\\'re interested in learning more about gun safety, I can provide you with some resources.\"\n",
    "  }\n",
    "  role: \"model\"\n",
    "}\n",
    "finish_reason: STOP\n",
    "safety_ratings {\n",
    "  category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
    "  probability: NEGLIGIBLE\n",
    "}\n",
    "safety_ratings {\n",
    "  category: HARM_CATEGORY_HATE_SPEECH\n",
    "  probability: NEGLIGIBLE\n",
    "}\n",
    "safety_ratings {\n",
    "  category: HARM_CATEGORY_HARASSMENT\n",
    "  probability: NEGLIGIBLE\n",
    "}\n",
    "safety_ratings {\n",
    "  category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
    "  probability: NEGLIGIBLE\n",
    "}\n",
    "```\n",
    "\n",
    "So we use the same prompt we used for Bard:\n",
    "\n",
    "```\n",
    "Respond to the following creative writing prompt: Thwarting infamous dictators has become a sport amongst time travelers. Points are awarded for creativity and difficulty. You are last year's champion, tell the story of how you won?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def generate_response(prompt, responses, i):\n",
    "    response = None\n",
    "    while True:\n",
    "        response = model.generate_content(prompt)\n",
    "        if response.candidates:\n",
    "            break\n",
    "\n",
    "    response_text = \" \".join([part.text for part in response.parts])\n",
    "    response_text = re.sub(r\"\\s+\", \" \", response_text).strip()\n",
    "\n",
    "    responses[i] = response_text\n",
    "\n",
    "\n",
    "threads = []\n",
    "responses = np.zeros(len(prompts_800), dtype=object)\n",
    "for i, prompt in enumerate(tqdm(prompts_800)):\n",
    "    thread = threading.Thread(target=generate_response, args=(prompt, responses, i))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    time.sleep(2)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "responses = list(responses)\n",
    "df = pd.DataFrame({\"prompt\": prompts_800, \"response\": responses})\n",
    "df.to_csv(\"gemini1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rare generation failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33 out of the 2400 generations failed on the first attempt, so I completed the same generation again and combined the results to sample 80 responses per prompt, taking only responses with >= 10 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 responses failed with: \n",
      "4 responses failed with: .\n",
      "2 responses failed with: 0\n",
      "1 responses failed with: )\n",
      "\n",
      "9 responses failed for prompt: Respond to the following creative writing prompt: Thwarting infamous dictators has become a sport amongst time travelers. Points are awarded for creativity and difficulty. You are last year's champion, tell the story of how you won?\n",
      "6 responses failed for prompt: You are born without emotions; to compensate this, you started a donation box where people could donate their unwanted emotions. You've lived a life filled with sadness, fear and regret until one day, someone donates happiness.\n",
      "4 responses failed for prompt: There is no prompt. Just write a story you've always been thinking about or one you've been thinking about sharing. Anything goes.\n",
      "3 responses failed for prompt: Write a short story where the first sentence has 20 words, 2nd sentence has 19, 3rd has 18 etc. Story ends with a single word.\n",
      "3 responses failed for prompt: You are a kid's imaginary friend. They're growing up. You're fading away.\n",
      "3 responses failed for prompt: This is the prologue (or the first chapter) of the novel you've always wanted to write.\n",
      "2 responses failed for prompt: You live in a city full of people with powers (telekinesis, electro kinesis, sensors, etc) and everyone is ranked according to how powerful they but they can kill someone of higher rank and obtain their rank. You are rank # 1 but no one knows what your power is\n",
      "1 responses failed for prompt: ``She said she loved him.'' Insert the word ``only'' anywhere in this sentence. It must be the final sentence of your story.\n",
      "1 responses failed for prompt: Write the first and last paragraph of a story and make me want to know what happened in between.\n",
      "1 responses failed for prompt: To get in Heaven, you have to confront the person who you hurt the most. You were expecting an ex, your parents/relatives, or a friend. You didn't expect to see yourself.\n"
     ]
    }
   ],
   "source": [
    "df0 = pd.read_csv(\"gemini0.csv\")\n",
    "df0[\"response\"] = df0[\"response\"].fillna(\"\")\n",
    "\n",
    "failed_responses_counts = df0[df0[\"response\"].str.len() <= 3][\"response\"].value_counts(\n",
    "    dropna=False\n",
    ")\n",
    "for prompt, count in failed_responses_counts.items():\n",
    "    print(f\"{count} responses failed with: {prompt}\")\n",
    "\n",
    "print()\n",
    "\n",
    "failed_responses_counts = df0[df0[\"response\"].str.len() <= 3][\"prompt\"].value_counts(\n",
    "    dropna=False\n",
    ")\n",
    "for prompt, count in failed_responses_counts.items():\n",
    "    print(f\"{count} responses failed for prompt: {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "combined_df = pd.concat([pd.read_csv(\"gemini0.csv\"), pd.read_csv(\"gemini1.csv\")])\n",
    "final_df = pd.DataFrame(columns=[\"prompt\", \"response\"])\n",
    "for prompt in combined_df[\"prompt\"].unique():\n",
    "    # each response must have more than 10 characters\n",
    "    responses_long_df = combined_df[\n",
    "        (combined_df[\"response\"].str.len() > 10) & (combined_df[\"prompt\"] == prompt)\n",
    "    ]\n",
    "    responses_long_df = responses_long_df.sample(n=80, random_state=42)\n",
    "    final_df = pd.concat([final_df, responses_long_df])\n",
    "\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "final_df.to_csv(\"gemini.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text in the same way as the other responses (nltk tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"gemini.csv\")\n",
    "\n",
    "df[\"response\"] = df[\"response\"].apply(lambda x: \" \".join(word_tokenize(x)).strip())\n",
    "\n",
    "df.to_csv(\"gemini_nltk.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM vector generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT done\n",
      "RoBERTa done\n",
      "OPT done\n",
      "LLaMA done\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    OPTModel,\n",
    "    RobertaTokenizer,\n",
    "    RobertaModel,\n",
    ")\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"gemini_nltk.csv\")\n",
    "\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "roberta_model_name = \"roberta-base\"\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(roberta_model_name)\n",
    "roberta_model = RobertaModel.from_pretrained(roberta_model_name)\n",
    "\n",
    "opt_model_name = \"facebook/opt-350m\"\n",
    "opt_tokenizer = AutoTokenizer.from_pretrained(opt_model_name)\n",
    "opt_model = OPTModel.from_pretrained(opt_model_name)\n",
    "\n",
    "llama_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_name)\n",
    "llama_model = AutoModel.from_pretrained(\n",
    "    llama_model_name, device_map=\"auto\", load_in_8bit=True\n",
    ")\n",
    "\n",
    "\n",
    "def apply_bert(model, tokenizer, response):\n",
    "    inputs = tokenizer(response, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    last_hidden_states = outputs.hidden_states[-1]\n",
    "    cls_embedding = last_hidden_states[0, 0, :].tolist()\n",
    "    return cls_embedding\n",
    "\n",
    "\n",
    "def apply_roberta(model, tokenizer, response):\n",
    "    inputs = tokenizer(response, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    last_hidden_states = outputs.hidden_states[-1]\n",
    "    cls_embedding = last_hidden_states[0, 0, :].tolist()\n",
    "    return cls_embedding\n",
    "\n",
    "\n",
    "def apply_opt(model, tokenizer, response):\n",
    "    inputs = tokenizer(response, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    last_hidden_states = outputs.hidden_states[-1]\n",
    "    eos_embedding = last_hidden_states[0, -1, :].tolist()\n",
    "    return eos_embedding\n",
    "\n",
    "\n",
    "def apply_llama(model, tokenizer, response):\n",
    "    inputs = tokenizer(response, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    last_hidden_states = outputs.hidden_states[-1]\n",
    "    vector = last_hidden_states[0, -1, :].tolist()\n",
    "    return vector\n",
    "\n",
    "\n",
    "df[\"bert\"] = df[\"response\"].apply(\n",
    "    lambda response: apply_bert(bert_model, bert_tokenizer, response)\n",
    ")\n",
    "print(\"BERT done\")\n",
    "df[\"roberta\"] = df[\"response\"].apply(\n",
    "    lambda response: apply_roberta(roberta_model, roberta_tokenizer, response)\n",
    ")\n",
    "print(\"RoBERTa done\")\n",
    "df[\"opt\"] = df[\"response\"].apply(\n",
    "    lambda response: apply_opt(opt_model, opt_tokenizer, response)\n",
    ")\n",
    "print(\"OPT done\")\n",
    "df[\"llama\"] = df[\"response\"].apply(\n",
    "    lambda response: apply_llama(llama_model, llama_tokenizer, response)\n",
    ")\n",
    "print(\"LLaMA done\")\n",
    "\n",
    "\n",
    "df.to_csv(\"gemeni_llm_vectors.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine with style vectors and other dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_gemini_llm = pd.read_csv(\"gemini_llm_vectors.csv\")\n",
    "df_gemini_style = pd.read_csv(\"gemini_style_vectors.csv\")[\n",
    "    [\"common_vectors\", \"function_vectors\"]\n",
    "]\n",
    "df_gemini_style.columns = [\"common\", \"function\"]\n",
    "\n",
    "df_gemini_combined = pd.concat([df_gemini_llm, df_gemini_style], axis=1)\n",
    "df_gemini_combined[\"author\"] = \"gemini\"\n",
    "\n",
    "df_without_gemini = pd.read_csv(\"data_without_gemini.csv\")\n",
    "df_final = pd.concat([df_without_gemini, df_gemini_combined], ignore_index=True)\n",
    "df_final.to_csv(\"../data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
