{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author\n",
      "reddit    800\n",
      "bard      800\n",
      "gpt       800\n",
      "0.8       200\n",
      "1.4       200\n",
      "1.3       200\n",
      "1.2       200\n",
      "1.1       200\n",
      "1.0       200\n",
      "0.9       200\n",
      "0.7       200\n",
      "0.6       200\n",
      "0.5       200\n",
      "0.4       200\n",
      "0.3       200\n",
      "0.2       200\n",
      "0.1       200\n",
      "1.5       200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "print(df[\"author\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def get_tvt_split(df_cur: pd.DataFrame):\n",
    "    grouped_indices = (\n",
    "        df_cur.groupby(\"prompt\")\n",
    "        .apply(lambda x: x.index.tolist(), include_groups=False)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    random.shuffle(grouped_indices)\n",
    "\n",
    "    test_indices = np.concatenate(grouped_indices[:2])\n",
    "\n",
    "    df_cur_test = df_cur.loc[test_indices]\n",
    "    df_cur_train = df_cur.drop(test_indices).reset_index(drop=True)\n",
    "\n",
    "    grouped_indices_train = (\n",
    "        df_cur_train.groupby(\"prompt\")\n",
    "        .apply(lambda x: x.index.tolist(), include_groups=False)\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    cv_iterable = []\n",
    "    for _ in range(4):\n",
    "        grouped_indices_train = grouped_indices_train[2:] + grouped_indices_train[:2]\n",
    "        val_indices = np.concatenate(grouped_indices_train[:2])\n",
    "        train_indices = np.concatenate(grouped_indices_train[2:])\n",
    "        cv_iterable.append((train_indices, val_indices))\n",
    "\n",
    "    return df_cur_train, df_cur_test, cv_iterable\n",
    "\n",
    "\n",
    "def get_best_params(X_train, y_train, cv_iterable):\n",
    "    param_grid = {\n",
    "        \"C\": [0.1, 0.3, 1, 3, 10],\n",
    "        \"solver\": [\"liblinear\", \"lbfgs\"],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        LogisticRegression(max_iter=1000, random_state=42),\n",
    "        param_grid,\n",
    "        cv=cv_iterable,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search.best_params_\n",
    "\n",
    "\n",
    "def expand_df(df, vector_names):\n",
    "    df_columns = []\n",
    "    for column in vector_names:\n",
    "        df_column = df[column].apply(ast.literal_eval).apply(pd.Series)\n",
    "        df_columns.append(df_column)\n",
    "    return pd.concat(df_columns, axis=1)\n",
    "\n",
    "\n",
    "def classify(authors, vector_names, n_trials=20):\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "\n",
    "    for random_state in range(n_trials):\n",
    "        df_authors = df[df[\"author\"].isin(authors)].reset_index(drop=True)\n",
    "        df_train, df_test, cv_iterable = get_tvt_split(df_authors)\n",
    "\n",
    "        X_train = expand_df(df_train, vector_names)\n",
    "        y_train = df_train[\"author\"]\n",
    "        X_test = expand_df(df_test, vector_names)\n",
    "        y_test = df_test[\"author\"]\n",
    "\n",
    "        standardizer = StandardScaler()\n",
    "        X_train = standardizer.fit_transform(X_train)\n",
    "        X_test = standardizer.transform(X_test)\n",
    "\n",
    "        best_params = get_best_params(X_train, y_train, cv_iterable)\n",
    "        model = LogisticRegression(\n",
    "            **best_params,\n",
    "            max_iter=1000,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return accuracies, f1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit, GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_vector_names = [\n",
    "    [\"common\"],\n",
    "    [\"function\"],\n",
    "    [\"common\", \"function\"],\n",
    "    [\"bert\"],\n",
    "    [\"common\", \"function\", \"bert\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT temp = Ben's chosen (default?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['common'] accuracy: 0.798 ± 0.104, f1: 0.786 ± 0.117\n",
      "['function'] accuracy: 0.797 ± 0.141, f1: 0.791 ± 0.146\n",
      "['common', 'function'] accuracy: 0.773 ± 0.176, f1: 0.765 ± 0.183\n",
      "['bert'] accuracy: 0.963 ± 0.026, f1: 0.963 ± 0.026\n",
      "['common', 'function', 'bert'] accuracy: 0.979 ± 0.029, f1: 0.979 ± 0.030\n"
     ]
    }
   ],
   "source": [
    "for vector_names in list_of_vector_names:\n",
    "    accuracies, f1s = classify([\"reddit\", \"gpt\"], vector_names)\n",
    "    print(\n",
    "        f\"{vector_names} accuracy: {np.mean(accuracies):.3f} ± {np.std(accuracies):.3f}, f1: {np.mean(f1s):.3f} ± {np.std(f1s):.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT temp $\\in [0.1,...,1.5]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit, Bard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bard, GPT (temp = Ben's chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trinary Classification (Reddit, GPT, Bard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
